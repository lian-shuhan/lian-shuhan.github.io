<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Python基础</title>
    <url>/2020/02/03/Python1/</url>
    <content><![CDATA[<h2 id="一、-简介"><a href="#一、-简介" class="headerlink" title="一、 简介"></a>一、 简介</h2><ol>
<li><p><strong>Python解释器</strong>: CPython</p>
</li>
<li><p><strong>Python交互模式</strong></p>
<ul>
<li><strong>进入</strong>: Python - - - - <strong>退出</strong>: exit()   </li>
<li><strong>运行文件</strong>: <code>python hello.py</code>(需在当前目录) <a id="more"></a>
</li>
</ul>
</li>
<li><p><strong>代码运行助手:</strong> <code>python learning.py</code>     </p>
</li>
<li><p><strong>编码</strong></p>
<ul>
<li>在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。</li>
<li>Python解释器按UTF-8编码读取: <code># -*- coding: utf-8 -*-</code></li>
</ul>
</li>
</ol>
<h2 id="二、-Python基础"><a href="#二、-Python基础" class="headerlink" title="二、 Python基础"></a>二、 Python基础</h2><ol>
<li><p><strong>列表</strong>: list[] - - - - <strong>元组</strong>: tuple()<br><strong>添加</strong>: x.add() - - - - <strong>删除</strong>: x.pop() - - - - <strong>插入</strong>: x.append()</p>
</li>
<li><p><strong>判断</strong>: if/elif/else<br><strong>循环</strong>: for/while - - - - 可迭代对象非常宽泛，用 <code>isinstance([], Iterable)</code> 判断</p>
</li>
<li><p><strong>字典</strong>: <pre>dict: d = {key1:value1,key2:value2}
d[key1]</pre><br><strong>set</strong>: <code>s = set([list])</code></p>
</li>
<li><p><strong>数据类型转换</strong>: <code>x = int/float/str/bool()</code></p>
</li>
<li><p><strong>函数</strong>: </p>
<ul>
<li>定义 <pre>def fun(x):
return y </pre></li>
<li>解决递归调用栈溢出的方法是通过<strong>尾递归优化</strong>，即在函数返回的时候，调用自身本身，并且，return语句不能包含表达式。</li>
</ul>
</li>
<li><p><strong>列表生成式</strong>:  <code>L = [x * x for x in range(1, 11) if x % 2 == 0]</code><br><strong>生成器</strong>: g = () - - - - 函数中即有yield语句<br><strong>迭代器</strong></p>
<ul>
<li>可以被next()函数调用并不断返回下一个值 </li>
<li>Iterator,即数据流/惰性序列 - - - - 把Iterable变成Iterator可以使用iter()函数</li>
<li>可用<code>list(Iterator)</code></li>
</ul>
</li>
</ol>
<h2 id="三、-函数式编程"><a href="#三、-函数式编程" class="headerlink" title="三、 函数式编程"></a>三、 函数式编程</h2><ol>
<li><p><strong>高阶函数</strong>: 以另一个函数作为参数的函数</p>
<ul>
<li><code>map(f，[x1, x2, x3, x4]) = Iterator</code></li>
<li><code>reduce(f, [x1, x2, x3, x4]) = f(f(f(x1,x2), x3), x4)</code></li>
<li><strong>filter() </strong>把函数依次作用于每个元素，根据返回值是True或False决定保留或丢弃该元素。</li>
<li><code>sorted([x1, x2, x3, x4], key = f)</code> 函数作用于每个元素，并根据函数返回结果进行排序</li>
</ul>
</li>
<li><p><strong>返回函数</strong>: 高阶函数把函数作为结果值返回。相关参数和变量都保存在返回的函数中，即“<strong>闭包</strong>”的程序结构</p>
</li>
<li><p><strong>匿名函数</strong>: <code>f = lambda x: x * x</code></p>
</li>
<li><p><strong>装饰器</strong>: 在代码运行期间动态增加函数的功能，本质是一个返回函数的高阶函数。</p>
<ul>
<li>@decorator 置于函数的定义上方: </li>
</ul>
</li>
<li><p><strong> 偏函数</strong>: <code>f1 = functools.partial(f, 参数 = 默认值)</code>,即把函数的某些参数设置默认值，返回一个新的函数。</p>
</li>
</ol>
<h2 id="四、-模块"><a href="#四、-模块" class="headerlink" title="四、 模块"></a>四、 模块</h2><ol>
<li><p>一个.py文件即是一个<strong>模块</strong>。<br><strong>包</strong>: 按目录来组织模块，每个包目录下都须含有名为 <strong>init</strong>.py 的文件(可为空)</p>
</li>
<li><p><code>if __name__==&#39;__main__&#39;:</code></p>
<ul>
<li>系统定义的名字要前后加下划线。</li>
<li>name是标识模块名字的系统变量。</li>
<li>若当前模块是主模块，那么此模块名字是main；若此模块是以模块形式被导入，则此模块名字为文件名字，通过if判断这样就会跳过后面的内容。</li>
</ul>
</li>
<li><p><strong>作用域</strong>: 通过_前缀来实现仅在模块内部使用的函数和变量。</p>
</li>
<li><p><strong>安装模块</strong>: <code>pip install 模块名</code>（pypi.python.org）/ <code>import 模块名</code></p>
</li>
<li><p><strong>Anaconda激活环境:</strong>: 输入<code>conda activate D:\Python\Anaconda1</code></p>
</li>
</ol>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python爬虫</title>
    <url>/2020/03/29/Python2/</url>
    <content><![CDATA[<h2 id="一、-Requests"><a href="#一、-Requests" class="headerlink" title="一、 Requests"></a>一、 Requests</h2><p><strong>自动爬取HTML页面，自动网络请求提交</strong></p>
<ol>
<li><p><strong>安装库</strong><br><pre>pip install requests
</pre> </p>
<a id="more"></a>
</li>
<li><p><strong>Requests库的主要方法</strong></p>
<ul>
<li>requests.request（method,url,**kwargs） </li>
<li><strong>requests.get()</strong> 获取HTML网页</li>
<li><strong>requests.head()</strong> 获取HTML网页头信息</li>
<li><strong>requests.post()</strong> 向HTML网页提交POST请求(即附加新数据)</li>
<li><strong>requests.put()</strong> 向HTML网页提交PUT请求(即修改且覆盖原数据)</li>
<li><strong>requests.patch()</strong> 向HTML网页提交局部修改请求</li>
<li><strong>requests.delete()</strong> 向HTML网页提交删除请求</li>
</ul>
</li>
<li><p><strong>get方法</strong><br><pre>r = requests.get(url，params=None,**kwargs)</pre><br>response对象中包含爬虫返回的内容，其属性:</p>
<ul>
<li><strong>r.status_code</strong> HTTP请求的返回状态(200为连接成功)</li>
<li><strong>r.text</strong> HTTP响应的字符串，即url的页面内容</li>
<li><strong>r.encoding</strong> 从HTTP header中猜测的响应内容编码方式(若header中不存在charset，则认为编码为ISO-8859-1)</li>
<li><strong>r.apparent_encoding</strong> 从内容中分析的响应内容编码方式(备选,真实)</li>
<li><strong>r.content</strong> HTTP响应内容的二进制形式<br>（<strong>r.requests.xxx</strong> 爬虫发给服务器的xxx信息）</li>
</ul>
</li>
<li><p><strong>爬取网页的通用代码框架</strong></p>
<pre>import requests
def getHTMLText(url):
     try:
         r = requests.get(url,timeout = 30)
         r.raise_for_status() #如果状态不是200.引发HTTPError异常
         r.encoding = r.apparent_encoding
         return r.text
     except:
         return "产生异常"
if __name__ == "__main__": 
     url = "http://www.baidu.com" 
     print(getHTMLText(url))</pre>
</li>
<li><p><strong>HTTP协议</strong>: 超文本传输协议,基于请求与响应模式、无状态的应用层协议(主要方法同上的requests功能一致)。<br><strong>URL</strong>是通过HTTP协议存取资源的Internet路径。</p>
</li>
</ol>
<h2 id="二、-robots-txt"><a href="#二、-robots-txt" class="headerlink" title="二、 robots.txt"></a>二、 robots.txt</h2><p><strong>网络爬虫排除标准</strong></p>
<ol>
<li><p>网络爬虫引发的问题: 骚扰问题；法律风险；隐私泄露</p>
</li>
<li><p>网络爬虫的限制: </p>
<ul>
<li><strong>来源审查</strong>：判断User-­Agent进行限制</li>
<li><strong>发布公告</strong>：Robots协议，即告知所有爬虫网站的爬取策略，要求爬虫遵守</li>
</ul>
</li>
<li><p><strong>Robots协议</strong>(即网络爬虫排除标准)</p>
<ul>
<li>在网站根目录下的robots.txt文件中</li>
<li>基本语法: <pre>User-agent: *
Disallow: /</pre></li>
</ul>
</li>
<li><p>例子</p>
<pre>kv = {'user‐agent':'Mozilla/5.0'} 
r = requests.get(url,headers=kv) # 模拟标准浏览器访问
/
kv = {'wd':'keyword'} 
r = requests.get("http://www.baidu.com/s",params=kv) # 向url中增加关键词内容
/
with open("path",'wb') as f:
f.write(r.content) #将爬取的内容写入本地path
</pre>

</li>
</ol>
<h2 id="三、-Beautiful-Soup"><a href="#三、-Beautiful-Soup" class="headerlink" title="三、 Beautiful Soup"></a>三、 Beautiful Soup</h2><p><strong>解析HTML页面</strong></p>
<ol>
<li><p>Beautiful Soup是解析、遍历、维护“标签树”(HTML/XML)的功能库</p>
</li>
<li><p><strong>解析器</strong>有: html.parser/lxml/xml/html5lib(后三种需安装对应的库)</p>
</li>
<li><p><strong>Beautiful Soup类的基本元素</strong>: Tag/name/attrs(属性，字典形式)/string/Comment（&lt;!）</p>
</li>
<li><p><strong>HTML内容遍历方法</strong></p>
<ul>
<li><strong>下行遍历</strong>: .contents(子节点列表);.chridren(子节点类型);.descendants(子孙节点迭代类型，如: <pre>for child in soup.a.descendants:
print(child)</pre></li>
<li><strong>上行遍历</strong>: .parent(父亲标签);.parents(先辈标签的迭代类型)</li>
<li><strong>平行遍历</strong>(同一个父节点下): .next_sibling;.previous_sibling;.next_siblings;.previous_siblings。</li>
</ul>
</li>
<li><p>编码: bs4库将demo皆转化为<strong>utf-8编码</strong>。</p>
</li>
<li><p><strong>信息标记</strong>的三种形式: XML(基于HTML)/JSON(有数据类型的键值对)/YAML(无类型的键值对)</p>
</li>
<li><p>信息提取的一般方法: 结合形式解析与搜索方法，提取关键信息-需要标记解析器及文本查找函数</p>
<pre>from bs4 import BeautifulSoup
soup = BeautifulSoup(demo,"html.parser") #对demo进行HTML解析
print(soup.prettify()) #格式输出
soup.a #输出a标签 
soup.a.parent.name #a的上一层标签的名字(元素)
soup.find_all(name,attrs,recursive,string,**kwargs) #显示对应(标签名、属性值,是否对儿节点以下搜索，字符串)的标签信息
soup.find_all(x = re.compile()) # 对x参数赋值正则表达式的方法值</pre>

</li>
</ol>
<h2 id="四、-Projects"><a href="#四、-Projects" class="headerlink" title="四、 Projects"></a>四、 Projects</h2><ol>
<li>中国大学排名定向爬虫</li>
</ol>
<pre>from bs4 import BeautifulSoup
import requests
import bs4

def getHTMLText(url):
    try:
        r = requests.get(url,timeout = 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""

def fillUnivList(ulist,html):
    soup = BeautifulSoup(html,"html.parser")
    for tr in soup.find('tbody').children:
        if isinstance(tr,bs4.element.Tag):
            tds = tr('td')
            ulist.append([tds[0].string, tds[1].string, tds[3].string])
    pass

def printUnivList(ulist,num):
    tplt = "{0:^10}\t{1:{3}^10}\t{2:^10}"
    print(tplt.format("排名","学校名称","总分",chr(12288)))
    for i in range(num):
        u=ulist[i]
        print(tplt.format(u[0], u[1], u[2],chr(12288)))

def main():
    uinfo = []
    url = 'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html'
    html = getHTMLText(url)
    fillUnivList(uinfo, html)
    printUnivList(uinfo, 20) #num = 20
main()</pre>

<h2 id="五、-Re"><a href="#五、-Re" class="headerlink" title="五、 Re"></a>五、 Re</h2><p><strong>正则表达式详解，提取页面关键信息</strong></p>
<ol>
<li><p>正则表达式: 通用的字符串表达框架。<br><strong>编译</strong>: 将一个正则表达式语法的字符串转化成一个特(即一个对象)，该特征对应一组字符串。</p>
</li>
<li><p><strong>语法</strong>-<strong>常用操作符</strong>:<br>.任何单个字符<br>[ ]对单字符给出取值范围<br><sup><a href="#fn_" id="reffn_"></a></sup>对单字符给出排除范围<br>*前一个字符出现0次或无限次<br>+前一个字符出现1次或无限次扩展<br>?前一个字符出现0次或1次<br>|左右表达式任意一个<br>{m}扩展前一个字符m次<br>{m,n}扩展前一个字符m至n次<br>^匹配字符串开头<br>$匹配字符串结尾<br>( )分组标记，内部只能使用 | 操作符<br>\d数字<br>\w单词字符</p>
</li>
<li><p><strong>Re库</strong>-标准库<br>正则表达式-<strong>raw string</strong>(即不含转义符)-表示为: r’text’<br>主要功能函数: </p>
<ul>
<li><strong>re.search(pattern,string,flags=0)</strong> 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象 </li>
<li><strong>re.match()</strong> 从一个字符串的开始位置起匹配正则表达式，返回match对象</li>
<li><strong>re.findall()</strong> 搜索字符串，以列表类型返回全部能匹配的子串 </li>
<li><strong>re.split(pattern,string,maxsplit=0,flags=0)</strong> 将一个字符串按照正则表达式匹配结果进行分割。返回列表类型(不匹配的) </li>
<li><strong>re.finditer()</strong> 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象 </li>
<li><strong>re.sub(pattern,repl,string,count=0,flags=0)</strong> 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串</li>
<li>两种用法：<br><pre>rst = re.search() # 函数式(一次性操作)<br>regex = re.complie(pattern，flags=0)#面向对象(编译后多次操作)<br>rst = regex.search(string) &lt;\pre&gt;</li>
</ul>
</li>
<li><p><strong>match对象</strong></p>
<ul>
<li>属性:<br><strong>.string</strong> 待匹配的文本<br><strong>.re</strong> 匹配时使用的pattern对象<br><strong>.pos</strong> 正则表达式搜索文本的开始位置<br><strong>.endpos</strong> 正则表达式搜索文本的结束位置 </li>
<li>方法:<br><strong>.group(0)</strong> 获得匹配后的字符串<br><strong>.start()</strong> 匹配字符串在原始字符串的开始位置<br><strong>.end()</strong> 匹配字符串在原始字符串的结束位置<br><strong>.span()</strong> 返回(.start(),.end())</li>
</ul>
</li>
<li><p>同时匹配长短不同的多项时：</p>
<ul>
<li><strong>贪婪匹配</strong>(Re库默认): 输出匹配最长的子串</li>
<li><strong>最小匹配</strong>: */+/?/{m,n}后加?</li>
</ul>
</li>
</ol>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据结构与算法</title>
    <url>/2020/04/07/Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="一、-算法分析"><a href="#一、-算法分析" class="headerlink" title="一、 算法分析"></a>一、 算法分析</h2><ol>
<li><p>赋值语句-包含计算和存储两个基本资源-可作为<strong>算法时间度量指标</strong>-时间复杂度大O，即O(f(n))</p>
</li>
<li><p>数据类型的性能</p>
<ul>
<li><strong>列表list</strong>: v = lst[i] ; lst.append(v) 为O(1)<br>lst = lst + [v] 为O(n+k)</li>
<li><strong>字典dict</strong></li>
</ul>
</li>
</ol>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Web前端开发基础</title>
    <url>/2020/04/08/Web%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<h2 id="一、-概论"><a href="#一、-概论" class="headerlink" title="一、 概论"></a>一、 概论</h2><ol>
<li><p>Web-Web系统；前端-网页上为用户呈现的部分；开发-编写代码。</p>
<a id="more"></a>
</li>
<li><p>服务器端: 网站-文件夹；网页-文件。<br>浏览器: 解析网页源代码，渲染网页。</p>
</li>
<li><p><strong>前端技术构成</strong></p>
<ul>
<li>结构-<strong>HTML</strong>: 从语义角度描述页面结构。</li>
<li>样式-<strong>CSS</strong>: 美化页面。</li>
<li>行为-<strong>JavaScript</strong>: 从交互角度提升用户体验。</li>
</ul>
</li>
</ol>
<h2 id="二、-HTML基础"><a href="#二、-HTML基础" class="headerlink" title="二、 HTML基础"></a>二、 HTML基础</h2><ol>
<li><p>HTML-超文本标记语言-<strong>HTML DOM</strong></p>
</li>
<li><p><strong>标签</strong></p>
<ul>
<li><code>h1</code>-<code>h6</code> 标题</li>
<li><code>p</code> 段落</li>
<li><code>&lt;br/&gt;</code> 段内换行 ；<code>&amp;nbsp</code> 空格字符</li>
<li><code>pre</code> 预留格式</li>
<li><code>span</code> 行内组合</li>
<li><code>&lt;hr /&gt;</code> 水平线</li>
<li><code>&lt;!--  --&gt;</code> 注释</li>
<li><code>&lt;a href=&quot;网址&quot;&gt;&lt;/a&gt;</code> 超链接</li>
<li><code>&lt;img src=&quot;相对路径&quot;/&gt;</code> 图片</li>
<li><code>div</code> <strong>区域</strong></li>
<li><code>ul</code> 无序列表/<code>ol</code> 有序列表&gt;<code>li</code> 列表项</li>
<li><code>table</code> 表格&gt;<code>tr</code> 行&gt;<code>td</code> 单元格/<code>th</code> 表头单元格</li>
<li><code>form</code> 表单&gt;<br><code>&lt;input type=&quot;text|password&quot; name=&quot;&quot; /&gt;</code> 文本框|密码框<br><code>&lt;input type=&quot;submit|reset&quot; value=&quot;&quot; /&gt;</code>提交按钮|重置按钮<br><code>&lt;input type=&quot;radio|checkbox&quot; value=&quot;值&quot; name=&quot;名称&quot; checked= /&gt;</code>单选框|复选框<br><code>select</code> 下拉列表&gt;<code>option selected=</code> 选项<br><code>textarea rows=行 cols=列</code> 文本域</li>
</ul>
</li>
<li><p>Web语义化-标签</p>
<ul>
<li>优点: 结构清晰、有利于搜索引擎理解-SEO搜索引擎优化、易兼容不同设备。</li>
<li><code>em</code> 斜体(强调)/<code>strong</code> 粗体(重点强调)</li>
<li><code>dl</code> 自定义列表&gt;<code>dt</code> 列表项&gt;<code>dd</code> 描述</li>
</ul>
</li>
</ol>
<h2 id="三、-CSS样式"><a href="#三、-CSS样式" class="headerlink" title="三、 CSS样式"></a>三、 CSS样式</h2><ol>
<li><p><strong>CSS添加方法</strong>: 行内样式&gt;内嵌样式&gt;链接样式(就近原则)<br><strong>CSS选择器</strong>: 标签/类别/ID选择器<br><strong>选择器的叠加用法</strong>: 嵌套/集体/全局声明</p>
</li>
<li><p><strong>文本</strong>:</p>
<ul>
<li>大小: px(像素)；em(字符)；%(百分比)</li>
<li>颜色: name；rgb()；rgba()；##rrggbb</li>
<li>属性: letter-spacing(字符间距)；line-height(行高)；text-align(对齐)；text-decoration(装饰线)；text-indent(首行缩进)</li>
</ul>
</li>
<li><p><strong>字体</strong>: font(斜体 粗体 字号/行高 字体)；font-family(字体系列)；font-size(字号)；font-style(斜体italic)；font-weight(粗体bold)</p>
</li>
<li><p><strong>背景</strong>: background-/color/image: url()/repeat: repeat-()</p>
</li>
<li><p><strong>超链接</strong>-伪类选择器: a:link/visited/hover/active</p>
</li>
<li><p><strong>列表标号</strong>: list-style-/image: url()/position/type</p>
</li>
<li><p><strong>表格</strong>: width,height/border(边框)/border-collapse(边框合并)/<code>:nth-child(odd|even)</code> 奇偶选择器</p>
</li>
</ol>
<h2 id="四、-CSS布局与定位"><a href="#四、-CSS布局与定位" class="headerlink" title="四、 CSS布局与定位"></a>四、 CSS布局与定位</h2><ol>
<li><p><strong>盒子模型</strong></p>
<ul>
<li>组成: content、height、width、padding、border、margin</li>
<li>内容溢出，使用overflow属性(hidden/scroll/auto)</li>
<li>border子属性: -width/style/color</li>
<li>margin属性垂直方向合并；水平居中用auto</li>
</ul>
</li>
<li><p><strong>CSS定位机制</strong></p>
<ul>
<li><strong>文档流定位</strong>(默认): 元素-block(独占一行且可设高宽)/inline(相反)/inline-block(不独占一行且可设高宽)；类型转换-display: none-</li>
<li><strong>浮动定位</strong>: float(浮动，即脱离文档流)；clear(清除浮动)</li>
<li><strong>层定位</strong>: position: fixed/relative(参照物为父元素，原位置仍存在)/absolute(参照物为最近的定义为相对/绝对的父元素，原位置不存在)<br>一般父元素相对定位，子元素绝对定位</li>
</ul>
</li>
</ol>
<h2 id="五、-CSS3"><a href="#五、-CSS3" class="headerlink" title="五、 CSS3"></a>五、 CSS3</h2><ol>
<li><p>浏览器内核-浏览器-<strong>CSS3前缀</strong>:<br>Webkit-Safari/Chrome- -webkit-<br>Gecko-Firefox- -moz-<br>Trident-IE- -ms-</p>
</li>
<li><p><strong>圆角边框</strong>: border- -radius: 水平 垂直<br><strong>阴影</strong>: box-shadow</p>
</li>
<li><p><strong>文本与文字</strong>的新属性:</p>
<ul>
<li>text-shadow 阴影</li>
<li>word-wrap: break-word 长单词、url分行</li>
<li>@font-face规则: 定义Web字体(在服务器端生成4种字体文件并引用)</li>
</ul>
</li>
<li><p><strong>2D变换</strong>-transform: rotate(deg)/scale(x,y)</p>
</li>
<li><p><strong>过渡</strong>(一般与超链接hover的变换合用): transition: property duration time-function<br><strong>动画</strong>: animation: name time time-function(@keyframes规则定义动画，关键帧{取值}）</p>
</li>
<li><p><strong>3D变换</strong>: transfrom-style: preserve-3d<br>transform: rotateX(deg)-父容器<br>perspective: -舞台</p>
</li>
</ol>
<h2 id="六、-JavaScript基础"><a href="#六、-JavaScript基础" class="headerlink" title="六、 JavaScript基础"></a>六、 JavaScript基础</h2><ol>
<li><p>JS: <strong>解释型脚本语言</strong>(运行前不编译，不检查错误)(编译型: C/C++)<br><strong>运行环境</strong>: NodeJS，浏览器内核中的JS解释器。<br><strong>JS组成</strong>: 核心(ECMAScript)，文档对象模型(DOM,与网页对话)，浏览器对象模型(BOM，与浏览器对话)。<br><strong>浏览器内核</strong>: 内容排版引擎(解析HTML和CSS)，脚本解释引擎(解析Javascript)<br><strong>程序</strong>=数据结构+算法。程序的结构: 顺序、分支、循环。<strong>算法</strong>: 解决问题的步骤。<strong>数据结构</strong>: 数据在内存中的存储结构。</p>
</li>
<li><p><strong>将JS脚本嵌入HTML</strong>的三种方法</p>
<ul>
<li><code>&lt;div id=&quot;&quot; 事件=JS代码&gt;xx&lt;/div&gt;</code></li>
<li><code>&lt;script&gt;JS代码&lt;/script&gt;</code>(可在任意位置)</li>
<li><code>&lt;script src=&quot;js文件路径&quot;&gt;&lt;/script&gt;</code>(在head中)</li>
</ul>
</li>
<li><p><strong>变量</strong></p>
<ul>
<li><strong>内存</strong>: 保存程序运行过程中所需的数据。8bit=1byte</li>
<li><strong>变量</strong>: 内存中的一段存储空间。(<strong>名</strong>: 内存空间的别名；<strong>值</strong>: 保存在内存空间中的数据)</li>
<li><strong>变量声明</strong>: <code>var 变量名=值</code>，为局部变量-console.log/document.write(变量名)=值(相当于print)</li>
</ul>
</li>
<li><p><strong>数据类型</strong>(弱类型)&amp;<strong>运算符</strong></p>
<ul>
<li><strong>基本类型</strong>: 数字(number)、字符串(“”-Unicode-2字节)、布尔、空、未定义、(NaN=Not a number)<br>转换分为隐式转换和强制转换()。</li>
<li><strong>引用类型</strong></li>
<li>三目运算符 表达式1?表达式2:表达式3;</li>
</ul>
</li>
<li><p><strong>函数</strong><br>声明 function 函数名(参数){可执行语句；return}<br>(要想使用函数内的全局变量，需先调用函数。)<br>(函数参数默认为局部变量)<br>(变量和函数声明提前，但赋值保留）<br>(传参的本质为全局变量复制副本给局部变量)</p>
</li>
<li><p><strong>分支结构</strong>: if-else(){}/switch(){case 值: 语句;break;}即做等值判断<br><strong>循环结构</strong>: while(){}/do{}while()/for(;;){}</p>
</li>
<li><p><strong>数组</strong>: 内存中连续存储多个数据的数据结构。</p>
<ul>
<li>索引数组: 创建 <code>var arr=[]/arr=new Array()</code> 长度: <code>arr.length</code></li>
<li>关联数组(hash，相当于字典): <code>arr[key]=元素值</code>(用hash[key]获取)</li>
<li><strong>数组API函数</strong>: String(arr)/arr.join(“”)/arr.concat()拼接/arr.slice(含头不含尾)选取/arr.splice(starti，n，添加值)删除/arr.reverse()颠倒/arr.sort</li>
</ul>
</li>
<li><p><strong>DOM</strong>: 文档对象模型，与平台和语言无关的应用程序接口(API)，可以动态地访问程序和脚本，并对其进行更新。<br><strong>DOM标准</strong>: 核心DOM；HTML DOM。</p>
<ul>
<li><strong>查找元素</strong><br><code>document.getElementById(&quot;id&quot;)</code><strong>最重要，用于JS代码中引入html元素，以便后续处理</strong><br><code>parent.getElementsByTagName(&quot;tag&quot;)</code><br><code>document.getElementsByName(&quot;name&quot;)</code><br><code>parent.getElementsByClassName(&quot;class&quot;)</code><br><code>parent.querySelector(All)(&quot;CSSselector&quot;)</code></li>
<li><strong>修改</strong>(<strong>核心DOM</strong>)<br>属性值:<code>elem.getAttribute(&quot;属性名&quot;)</code>-读取<br><code>elem.setAttribute(&quot;属性名&quot;,value)</code>-修改<br><code>elem.hasAttribute(&quot;属性名&quot;)</code>-判断<br><code>elem.removeAttribute(&quot;属性名&quot;)</code>-移除<br>样式: <code>elem.style.属性名</code>(内联，去横杠变大写)</li>
<li><strong>添加元素</strong>:<br><code>elem=document.createElement(&quot;元素名&quot;)</code>-创建空元素<br><code>a.属性名=&quot;属性值&quot;</code>-设置属性<br><code>a.style.样式名(cssText)=&quot;&quot;</code>-设置样式<br><code>parentNode.appendChild(childNode)</code>/<code>parentNode.insertBefore(newChild，existingChild)</code>-添加到DOM树(优化: 使用文档片段)</li>
</ul>
</li>
<li><p><strong>BOM</strong>: 浏览器对象模型，专门操作浏览器窗口的API。<br>定时器: 周期性<code>timer=setInterval(exp,time)</code> 停止<code>clearInterval(timer)</code><br>一次性: <code>setTimeout(exp,time)</code></p>
</li>
</ol>
<h2 id="七、-JQuery"><a href="#七、-JQuery" class="headerlink" title="七、 JQuery"></a>七、 JQuery</h2><ol>
<li><p><strong>JQuery概述</strong>: 对DOM操作的终极简化<br>使用: 下载JQuery.js到本地，再通过script引入/使用CDN网络中共享的JQuery.js。再引入我们所编写的JQery代码的JS文件。<br><strong>工厂函数</strong>: 选择符$()，一般为$(“标签名/.class值/#id值”)</p>
</li>
<li><p><strong>JQuery增删改查</strong></p>
<ul>
<li>查找元素-兄弟关系: $(“…”).next/prev/nextAll/prevAll/siblings()，其他同CSS</li>
<li>读取/修改(对属性): <code>$(&quot;&quot;).attr(&quot;标签属性名&quot;,值)</code></li>
<li>读取/修改(对内容):<br><code>$(&quot;&quot;).html(&quot;&quot;)</code> html操作<br><code>html/$(&quot;&quot;).text(&quot;&quot;)</code> 文本操作<br><code>$(&quot;&quot;).val(&quot;&quot;)</code> 值操作</li>
<li>读取/修改(对样式):<br><code>$(&quot;&quot;).css(&quot;CSS属性名,值&quot;)</code>直接修改css属性<br><code>$(&quot;&quot;).hasClass(&quot;类名，即class&quot;)</code>/<code>$(&quot;&quot;).addClass()</code>/<code>$(&quot;&quot;).removeClass()</code> 通过修改class</li>
<li>添加元素: <code>var $newelem=$(&quot;html代码&quot;)</code>，<code>$(parent).append($newelem)</code></li>
<li>删除: <code>$(&quot;&quot;).remove(&quot;&quot;)</code>，(如选第二个li <code>$(&quot;ul li:eq(1)&quot;</code>)</li>
</ul>
</li>
<li><p><strong>JQuery事件</strong>-事件绑定: <code>$(&quot;&quot;).bind(&quot;事件类型&quot;，function(e)&#123;..&#125;)</code>，e为<strong>事件对象</strong></p>
</li>
</ol>
<h2 id="八、-响应式布局-Bootstrap"><a href="#八、-响应式布局-Bootstrap" class="headerlink" title="八、 响应式布局-Bootstrap"></a>八、 响应式布局-Bootstrap</h2><ol>
<li><p><strong>布局方式</strong>(宽度): 固定宽度布局(960/980/1190/1210px)、流式布局(屏幕百分比设置相对宽度)、响应式布局(根据设备视觉视口进行不同的布局)<br>设备屏幕尺寸划分: 768/992/1200</p>
</li>
<li><p>响应式布局的<strong>实现方式</strong>: </p>
<ul>
<li>CSS3中的媒体查询-检查设备宽度，设置CSS样式<br>meta:vp扩展-布局视口=设备视觉视口，不可缩放<br>style&gt;@media screen and (min-width: px){CSS选择器}-媒体查询</li>
<li>开源框架(Bootstrap)</li>
</ul>
</li>
<li><p><strong>Bootstrap</strong></p>
<ul>
<li>概述: 开源、移动优先的前端框架。</li>
<li>使用: 下载后引用/CDN直接引用-CSS和JS文件<br>引用jQuery: <code>&lt;script src=&quot;https://cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js&quot;&gt;&lt;/script&gt;</code></li>
<li>栅格系统: .container&gt;.row&gt;.col-xs/sm/md/lg-12</li>
</ul>
</li>
</ol>
<h2 id="九、-HTML5"><a href="#九、-HTML5" class="headerlink" title="九、 HTML5"></a>九、 HTML5</h2><ol>
<li><p><strong>新结构</strong></p>
<ul>
<li>旧页面-HTML4标准: div+css</li>
<li>HTML5-新标签: header/footer/nav/aside/content/section/article…</li>
</ul>
</li>
<li><p><strong>音频视频</strong></p>
<ul>
<li>音频 <code>&lt;audio src=&quot;&quot; controls=&quot;controls&quot;&gt;..&lt;/audio&gt;</code></li>
<li>视频<br><code>&lt;video id=&quot;&quot; width=&quot;&quot;&gt;&lt;source src=&quot;&quot; typy=&quot;video/mp4&quot;/&gt;..&lt;/video&gt;&lt;button onclick=&quot;playPause()&quot;&gt;播放/暂停&lt;/button&gt;</code><br>(用JS定义playPause函数)</li>
</ul>
</li>
<li><p><strong>绘图Canvas</strong>： 用Canvas标签定义画布和设置画布样式(CSS)，用JS准备绘图上下文环境和JS绘图</p>
</li>
</ol>
<h2 id="十、-实战笔记"><a href="#十、-实战笔记" class="headerlink" title="十、 实战笔记"></a>十、 实战笔记</h2><ol>
<li><p>一般来说，id用于布局，class用于样式。</p>
</li>
<li><p><code>margin: 0px auto</code> 指块元素在父元素中水平居中对齐。<br><code>text align: center</code> 指块元素中的内联元素在块元素内部水平居中。也可对父容器设置。</p>
</li>
<li><p>内联元素无法设置宽高-<code>display: inline-block</code></p>
</li>
<li><p>网页先加载html(img标签-内容图片)，再加载css文件(css背景图片样式-小图标、背景图片)</p>
</li>
<li><p>一般步骤: 设置content高和背景颜色，container(可舍去)，设置子层高宽和边框</p>
</li>
<li><p>若上下盒子有空隙，可对下盒子清除浮动(<code>overflow: hidden</code>)</p>
</li>
<li><p>可引入swiper(轮播图)、animate(动画效果)做网页插件。<br>方法: 先在html中引入本地的对应css/js文件，再加CSS和JS代码。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>技能</tag>
      </tags>
  </entry>
  <entry>
    <title>绪论-机器学习</title>
    <url>/2020/02/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/</url>
    <content><![CDATA[<h2 id="一、-基本术语"><a href="#一、-基本术语" class="headerlink" title="一、 基本术语"></a>一、 基本术语</h2><ol>
<li><p>若欲预测的是离散值，此类学习任务称为”<strong>分类</strong>“; 若欲预测的是连续值，此类学习任务称为”<strong>回归</strong>“。</p>
</li>
<li><p>学得模型后，使用其进行预测的过程称为”<strong>测试</strong>“。</p>
<a id="more"></a>
</li>
<li><p>将训练样本分成若干组，每组称为一个”<strong>簇</strong>“; 这些自动形成的簇可能对应一些潜在的概念划分。在<strong>聚类学习</strong>中，这种概念我们事先是不知道的，并且训练样本通常不拥有标记信息。</p>
</li>
<li><p>根据训练数据是否拥有标记信息，学习任务可大致划分为两大类: <strong>监督学习</strong>(分类，回归)；<strong>无监督学习</strong>(聚类)。</p>
</li>
<li><p>学得模型适用于新样本的能力，称为”<strong>泛化</strong>“ 能力。</p>
</li>
<li><p>归纳学习-概念学习-布尔概念学习<strong>（0/1）</strong> </p>
</li>
</ol>
<h2 id="二、-假设空间"><a href="#二、-假设空间" class="headerlink" title="二、 假设空间"></a>二、 假设空间</h2><ol>
<li><p>把学习过程看作在假设空间中进行搜索的过程，搜索目标是找到与训练集”匹配”的假设，即能够将训练集判断正确的假设（不断删除与正例不一致的假设）。</p>
</li>
<li><p>可能有多个假设与训练集一致，即存在着一个与训练集一致的”假设集合”，我们称之为”<strong>版本空间</strong>“。</p>
</li>
</ol>
<h2 id="三、-归纳偏好"><a href="#三、-归纳偏好" class="headerlink" title="三、 归纳偏好"></a>三、 归纳偏好</h2><ol>
<li><p>针对版本空间，机器学习算法在学习过程中对某种类型假设的偏好，称为”<strong>归纳偏好</strong>“, 或简称为”偏好”。</p>
</li>
<li><p>归纳偏好可看作学习算法自身在一个庞大的假设空间中对假设进行选择的启发式或”价值观”。即什么样的模型更好。 算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好性能。</p>
</li>
<li><p><strong>奥卡姆剃刀</strong>: 若有多个假设与观察一致，则选最简单的那个。</p>
<p>它是最常见的最基本的引导算法确立“正确的偏好”的一般性原则。</p>
</li>
<li><p><strong>NFL定理</strong>: 由于对所有可能函数的相互补偿，最优化算法的性能是等价的。即没有一个学习算法可以在任何领域总是产生最准确的学习器。</p>
<ul>
<li>不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。</li>
<li>在脱离实际意义情况下，空泛地谈论哪种算法好毫无意义，要谈论算法优劣必须针对具体学习问题。在某些问题上表现好的学习算法，在另一些问题上却可能不尽如人意，学习算法自身的归纳偏好与问题是否相配，往往会起到决定性的作用.</li>
</ul>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>降维与度量学习-机器学习</title>
    <url>/2020/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A010/</url>
    <content><![CDATA[<h2 id="一、-k近邻学习"><a href="#一、-k近邻学习" class="headerlink" title="一、 k近邻学习"></a>一、 k近邻学习</h2><ol>
<li><a id="more"></a></li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>模型评估与选择-机器学习</title>
    <url>/2020/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02/</url>
    <content><![CDATA[<h2 id="一、-经验误差与过拟合"><a href="#一、-经验误差与过拟合" class="headerlink" title="一、 经验误差与过拟合"></a>一、 经验误差与过拟合</h2><ol>
<li><p><strong>错误率</strong>: 分类错误的样本数占样本总数的比例。</p>
<p> <strong>精度</strong> = 1-错误率</p>
<a id="more"></a>
</li>
<li><p><strong>误差</strong>: 实际预测输出与样本的真实输出之间的差异。</p>
<p> <strong>训练集</strong>-训练/经验误差。</p>
<p> <strong>新样本</strong>-泛化误差(目标)</p>
</li>
<li><p><strong>过拟合</strong>: 当学习器把训练样本学得”太好”时(学习能力过强，经验误差过小)，很可能已把训练样本自身所包含的不太一般的特性当作了所有潜在样本都会具有的一般性质，从而导致泛化性能下降。(关键障碍，无法彻底避免)</p>
</li>
<li><p><strong>欠拟合</strong>: 对训练样本的一般性质尚未学好。(易克服)</p>
</li>
</ol>
<h2 id="二、-模型评估方法"><a href="#二、-模型评估方法" class="headerlink" title="二、 模型评估方法"></a>二、 模型评估方法</h2><ol>
<li><p>使用”<strong>验证集</strong>“来测试学习器对新样本的判别能力，然后以验证集的”测试误差”作为泛化误差的近似。<br>验证集应该尽可能与训练集互斥，即通过对原数据集进行适当处理，从中产生出训练集和验证集。</p>
</li>
<li><p><strong>留出法</strong>: 直接将数据集划分为两个互斥的集合(70%训练集，30%验证集)。一般用多次随机划分，重复实验后取均值作为留出法的评估结果。</p>
<p> <strong>分层采样</strong>: 保留训练集和验证集的类别比例。</p>
</li>
<li><p><strong>交叉验证法</strong>(k折交叉验证): 先将数据集通过分层采样分为k个大小相似的互斥子集，每次用k-1子集的并集作为训练集,余下的那个子集作为验证集，即有k组训练和验证集，从而进行k次训练和测试，最终返回这k个测试结果的均值。 k常取10。<br>通常要随机使用不同的划分重复p次,即p次k折交叉验证。</p>
<p> <strong>留一法</strong>: k = 样本数 （每个子集包含一个样本)</p>
</li>
<li><p><strong>自助法</strong>-自助采样: 每次随机从原数据集中挑选一个样本，将其拷贝放入采样数据集，然后再将该样本放回初始数据集，重复m次，得到训练集。未在训练集中出现的样本用于验证集(m无穷时，样本比例为0.368)，测试结果称为”包外估计”。(数据集较小时有用)</p>
</li>
<li><p><strong>调参</strong>: 对每个参数选定一个范围和变化步长，最终从多个候选值中产生选定值。</p>
</li>
<li><p><strong>最终模型</strong>: 模型选择完成，算法和参数选定后，应再用原数据集(所有样本)重新训练模型。</p>
</li>
<li><p><strong>测试集</strong>-测试数据: 学得模型在实际使用中遇到的数据。</p>
<p><strong>训练数据</strong> = 训练集 + 验证集 ，基于验证集上的性能进行模型选择和调参。</p>
</li>
</ol>
<h2 id="三、-性能度量"><a href="#三、-性能度量" class="headerlink" title="三、 性能度量"></a>三、 性能度量</h2><ol>
<li><p><strong>性能度量</strong>: 衡量模型泛化能力的评价标准。反映了任务需求。给定样例集$D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$,将把学习器预测结果$f(x)$与真实标记$y$进行比较,以此评估学习器$f$的性能。</p>
</li>
<li><p><strong>错误率</strong>: </p>
<script type="math/tex; mode=display">E(f ; \mathcal{D})=\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x}) \neq y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}</script><p><strong>精度</strong>: </p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{acc}(f ; D) &=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right)=y_{i}\right) \\
&=1-E(f ; D)
\end{aligned}</script></li>
<li><p><strong>查全率、查准率与F1</strong><br><img src="https://s2.ax1x.com/2020/02/17/3PYpTg.png" alt="3PYpTg.png"></p>
<ul>
<li>P-R曲线: 根据学习器的预测结果（置信度）对样例进行排序，按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率R（横轴）和查准率P（纵轴）。</li>
<li>平衡点(BEP): 查全率 = 查准率(越高越好)</li>
<li>F1度量(常用)-$F_{\beta}$<script type="math/tex; mode=display">F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}</script>$\beta$越大(&gt;1)，查全率有更大影响。</li>
<li>n个二分类混淆矩阵: 先计算再平均（宏），先平均再计算(微)</li>
</ul>
</li>
<li><p><strong>ROC与AUC</strong></p>
<ul>
<li>ROC(受试者工作特征)图: 假正例率FPR(横轴)，真正例率TPR(纵轴)<script type="math/tex; mode=display">\begin{aligned}
&\mathrm{TPR}=\frac{T P}{T P+F N}\\
&\mathrm{FPR}=\frac{F P}{T N+F P}
\end{aligned}</script></li>
<li>AUC: ROC曲线下方的面积。(越大越好)</li>
<li>排序损失 = 1 - AUC</li>
</ul>
</li>
<li><p><strong>代价敏感错误率与代价曲线</strong></p>
<ul>
<li>不同类型的错误所造成的后果不同(非均等代价)</li>
<li>代价曲线-假反例率FNR=1-TPR-ROC-曲线上每点对应其一条线段，所有线段的下界即期望总体代价<br><img src="https://s2.ax1x.com/2020/02/18/3kkkuR.png" alt="3kkkuR.png"></li>
</ul>
</li>
</ol>
<h2 id="四、-比较检验"><a href="#四、-比较检验" class="headerlink" title="四、 比较检验"></a>四、 比较检验</h2><ol>
<li><p><strong>统计假设检验</strong>对性能度量结果进行比较(本节默认以错误率$\epsilon$为性能度量)。  </p>
</li>
<li><p><strong>假设检验</strong>: 一次留出法用<strong>二项检验</strong>，多次/交叉用<strong>t检验</strong></p>
</li>
<li><p><strong>交叉验证t检验</strong>: 对于两个使用相同k折交叉验证法的学习器，根据k对结果(即错误率)的差值对”学习器性能相同”假设做t检验。<br>若不同轮次训练集有重叠-测试错误率不独立-假设成立概率过高估计-采用<strong>$5 \times 2$交叉验证</strong></p>
</li>
<li><p><strong>McNemar 检验</strong>: 由两学习器分类差别列联表的e10和e01对”学习器性能相同”假设做卡方分布检验。</p>
</li>
<li><p><strong>Friedman检验与Nemenyi后续检验</strong>: 在一组数据集上对多个算法进行比较(算法比较序值表)。<br>“所有算法的性能相同”-后续检验进一步区分各算法</p>
</li>
</ol>
<h2 id="五-偏差与方差"><a href="#五-偏差与方差" class="headerlink" title="五. 偏差与方差"></a>五. 偏差与方差</h2><ol>
<li><p><strong>偏差方差分解</strong>是解释学习算法为何具有泛化性能的工具-对期望泛化错误率进行拆解。</p>
</li>
<li><p>推导</p>
<script type="math/tex; mode=display">E(f ; D)=\mathbb{E}_{D}\left[\left(f(\boldsymbol{x} ; D)-y_{D}\right)^{2}\right]</script><script type="math/tex; mode=display">=\mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x}))^{2}\right]+(\bar{f}(\boldsymbol{x})-y)^{2}+\mathbb{E}_{D}\left[\left(y_{D}-y\right)^{2}\right]</script><script type="math/tex; mode=display">E(f ; D)=\operatorname{bias}^{2}(\boldsymbol{x})+\operatorname{var}(\boldsymbol{x})+\varepsilon^{2}</script></li>
<li><p><strong>期望泛化误差</strong>可分解为偏差、方差与噪声之和。<br>即泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。</p>
</li>
<li><p><strong>偏差一方差窘境</strong><br><img src="https://s2.ax1x.com/2020/02/19/3EPhrR.png" alt="3EPhrR.png"></p>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>线性模型-机器学习</title>
    <url>/2020/02/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A03/</url>
    <content><![CDATA[<h2 id="一、-线性回归"><a href="#一、-线性回归" class="headerlink" title="一、 线性回归"></a>一、 线性回归</h2><ol>
<li><p>输入离散属性: 存在”序”关系-<strong>连续化 </strong>；不存在”序”关系-转化为k维向量。</p>
</li>
<li><p>回归中最常用的性能度量: <strong>均方误差</strong>-欧氏距离-最小二乘”参数估计”-求偏导等于0-最优解的闭式解（解析解）</p>
<a id="more"></a>
<ul>
<li>一元: $E_{(w, b)}=\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}$</li>
<li>多元: $E_{\hat{w}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$，其中$\hat{\boldsymbol{w}}=(\boldsymbol{w} ; b)$，<strong>X</strong>加上值=1的最后一列。</li>
</ul>
</li>
<li><p>广义线性函数-联系函数g()-g()=ln()时，为<strong>对数线性回归模型</strong>-参数估计: 加权最小二乘法/极大似然法</p>
<script type="math/tex; mode=display">
y=g^{-1}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)</script></li>
</ol>
<h2 id="二、-对数几率回归"><a href="#二、-对数几率回归" class="headerlink" title="二、 对数几率回归"></a>二、 对数几率回归</h2><ol>
<li><p><strong>分类任务</strong>(二分类): 找一个单调可微函数g，将分类任务的真实标记y与线性回归模型的预测值相联系。</p>
</li>
<li><p><strong>对数几率函数</strong>: $y=\frac{1}{1+e^{-z}}$<br>对数几率: $\ln \frac{y}{1-y}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$ （将y视为样本x作为正例的可能性）</p>
</li>
<li><p>确定w和b: 将y视为类后验概率估计$p(y=1 | \boldsymbol{x})$(已知类别后，属于该类的概率)，用极大似然法:</p>
<script type="math/tex; mode=display">
\ell(\boldsymbol{w}, b)=\sum_{i=1}^{m} \ln p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)</script><p>将似然项以0/1概率拆分为通用形式后，再用凸优化理论或数值优化算法求其最优解。</p>
</li>
</ol>
<h2 id="三、-线性判别分析"><a href="#三、-线性判别分析" class="headerlink" title="三、 线性判别分析"></a>三、 线性判别分析</h2><ol>
<li><p><strong>线性判别分析</strong>(LDA)，亦称Fisher判别分析: 将训练样例投影到一条直线（多维）上，使同类的投影点尽可能接近、异类的远离;对新样本分类时，将其投影到同样的直线上，根据投影点的位置来确定其类别。</p>
</li>
<li><p><strong>最大化目标</strong>（二分类）: 类中心距离（大）/同类的投影点的协方差(小)，设直线为$y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}$，均值向量$\mu_{i}$，协方差矩阵$\mathbf{\Sigma}_{i}$，$i \in\{0,1\}$（类别），即</p>
<script type="math/tex; mode=display">
\begin{aligned}
J &=\frac{\left\|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{0}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{1}\right\|_{2}^{2}}{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{0} \boldsymbol{w}+\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{1} \boldsymbol{w}} \\
&=\frac{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\Sigma}_{0}+\boldsymbol{\Sigma}_{1}\right) \boldsymbol{w}}
\end{aligned}</script><p>定义类内散度矩阵$\mathbf{S}_{w}$，类间散度矩阵$\mathbf{S}_{b}$，则$J=\frac{\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{b} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{w} \boldsymbol{w}}$，LDA最大化的目标即是二者的广义瑞利商。<br>确定$\boldsymbol{w}$: 令分母为1（解与$\boldsymbol{w}$长度无关），用拉格朗日乘子法求解（$\mathbf{S}_{b} \boldsymbol{w}$方向恒为$\mu_{0}-\mu_{1}$）。</p>
</li>
<li><p>当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类。</p>
</li>
<li><p>多分类LDA-监督降维。</p>
</li>
</ol>
<h2 id="四、-多分类学习"><a href="#四、-多分类学习" class="headerlink" title="四、 多分类学习"></a>四、 多分类学习</h2><ol>
<li><p><strong>拆解法</strong>: 先对多分类任务进行拆分，然后为拆出的每个二分类任务训练一个分类器，最后在测试时将预测结果进行集成。<br>本节介绍拆分策略:一对一，一对其余，多对多。</p>
</li>
<li><p><strong>一对一</strong>(OvO): 将多类别两两配对(重复)训练分类器，测试时将新样本提交给所有分类器，最后通过投票产生最终分类结果。</p>
</li>
<li><p><strong>一对其余</strong>(OvR): 将一个类(正例)和所有其他类(反例)配对来训练(N个分类器)。测试时若仅一个分类器为正,则对应为最终分类预测结果；反之则选择置信度大的分类器的类别标记。</p>
</li>
<li><p><strong>多对多</strong>(MvM): 每次将若干个类作为正类，若干个其他类作为反类。<br>正反类构造: 纠错输出码(ECOC)</p>
</li>
</ol>
<h2 id="五、-类别不平衡问题"><a href="#五、-类别不平衡问题" class="headerlink" title="五、 类别不平衡问题"></a>五、 类别不平衡问题</h2><ol>
<li><p><strong>类别不平衡</strong>: 不同类别的训练样例数目差别很大。本节假设正例少，反例多。</p>
</li>
<li><p>线性分类器-<strong>再缩放</strong>:<br>新样本的正例与反例可能性之比值(分类器预测几率): $\frac{y^{\prime}}{1-y^{\prime}}=\frac{y}{1-y} \times \frac{m^{-}}{m^{+}}$,其中$m^{+}$为训练集中正例数目，$m^{-}$为反例数目。<br>最终: 分类器预测几率高于观测几率(缩放前为0.5)，即判定为正例。</p>
</li>
<li><p>若训练集不是真实样本总体的无偏采样-对训练集中反例欠采样/对正例过采样/阈值移动(在决策时进行再缩放)</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-机器学习</title>
    <url>/2020/03/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A05/</url>
    <content><![CDATA[<h2 id="一、-神经元模型"><a href="#一、-神经元模型" class="headerlink" title="一、 神经元模型"></a>一、 神经元模型</h2><ol>
<li><p><strong>M-P神经元模型</strong>: 阈值逻辑单元-有激活函数-功能神经元<br><img src="https://s2.ax1x.com/2020/03/03/3hykiq.png" alt="3hykiq.png"></p>
<a id="more"></a>
</li>
<li><p><strong> 激活\响应函数</strong>: 阶跃函数(理想)<br> Sigmoid(挤压)函数: $\frac{1}{1+e^{-x}}$</p>
</li>
</ol>
<h2 id="二、-感知机与多层网络"><a href="#二、-感知机与多层网络" class="headerlink" title="二、 感知机与多层网络"></a>二、 感知机与多层网络</h2><ol>
<li><p><strong>感知机</strong>: 两层神经元(输入层，输出层M-P)</p>
<ul>
<li>给定训练集，通过学习得到权重$w_{i}$(阈值$\theta$看做输入值为-1的哑结点对应的权重)</li>
<li>学习规则-对训练样例(x,y),若感知机输出为$\hat{y}$，则权重调整:<script type="math/tex; mode=display">
\begin{aligned}
&w_{i} \leftarrow w_{i}+\Delta w_{i}\\
&\Delta w_{i}=\eta(y-\hat{y}) x_{i}
\end{aligned}</script>$\eta$为学习率</li>
<li>只能处理线性可分问题</li>
</ul>
</li>
<li><p><strong>多层前馈神经网络</strong>: 中间有隐含层(有激活函数)<br><strong>神经网络的学习过程</strong>: 根据训练数据来调整神经元之间的连接权(即权重)以及每个功能神经元的阈值。</p>
</li>
</ol>
<h2 id="三、-误差逆传播-BP-算法-反向算法"><a href="#三、-误差逆传播-BP-算法-反向算法" class="headerlink" title="三、 误差逆传播(BP)算法/反向算法"></a>三、 误差逆传播(BP)算法/反向算法</h2><ol>
<li><p><strong>BP网络</strong>-一般指用BP算法(一种迭代学习算法)训练的多层前馈神经网络。<br><img src="https://s2.ax1x.com/2020/03/04/3oBEVJ.png" alt="3oBEVJ.png"></p>
<ul>
<li>m个样本，d个属性，y为$l$维向量，激活函数为Sigmoid函数</li>
<li>第k个样本的均方误差: $E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(\hat{y}_{j}^{k}-y_{j}^{k}\right)^{2}$</li>
</ul>
</li>
<li><p><strong>标准BP算法的推导</strong>: 确定隐层和输出层的权值和阈值(每次更新仅针对训练集中的一个样例)</p>
<ul>
<li>在迭代的每一轮采用广义感知机学习规则对参数进行更新估计，即$v \leftarrow v+\Delta v$)</li>
<li>基于梯度下降策略，以目标的负梯度方向对参数进行调整。</li>
<li><strong>更新公式</strong>(给定学习率$\eta$，常为0.1): <script type="math/tex; mode=display">
\begin{array}{l}
\Delta w_{h j}=\eta g_{j} b_{h} \\
\Delta \theta_{j}=-\eta g_{j} \\
\Delta v_{i h}=\eta e_{h} x_{i} \\
\Delta \gamma_{h}=-\eta e_{h}
\end{array}</script></li>
<li>其中输出层神经元的梯度项$g_{j}=-\frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}}=\hat{y}_{j}^{k}\left(1-\hat{y}_{j}^{k}\right)\left(y_{j}^{k}-\hat{y}_{j}^{k}\right)$<br>隐层神经元的梯度项$e_{h}=-\frac{\partial E_{k}}{\partial b_{h}} \cdot \frac{\partial b_{h}}{\partial \alpha_{h}}=b_{h}\left(1-b_{h}\right) \sum_{j=1}^{l} w_{h j} g_{j}$</li>
</ul>
</li>
<li><p><strong>BP算法的工作流程</strong>: 一样本传入输入层-产生输出层结果和误差-将误差逆向传播至隐层-计算梯度项-根据更新公式调整连接权和阈值-迭代过程循环进行-达到某停止条件(如训练误差很小)</p>
</li>
<li><p><strong>累积BP算法</strong>: 最小化训练集的累积误差$E=\frac{1}{m} \sum_{k=1}^{m} E_{k}$<br>在读取整个训练集一遍后才对参数进行更新。</p>
</li>
<li><p>只需一个包含足够多神经元的隐层(个数的确定用试错法调整)，多层前馈网络就能以任意精度逼近任意复杂度的连续函数。</p>
</li>
<li><p>BP神经网络常会<strong>过拟合</strong>，解决办法有:</p>
<ul>
<li><strong>早停</strong>: 训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差降低但验证集升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。</li>
<li><strong>正则化</strong>: 在误差目标函数中增加一个用于描述网络复杂度的部分(从而使网络输出更加”光滑”)。</li>
</ul>
</li>
</ol>
<h2 id="四、-全局最小与局部极小"><a href="#四、-全局最小与局部极小" class="headerlink" title="四、 全局最小与局部极小"></a>四、 全局最小与局部极小</h2><ol>
<li><p>神经网络的训练过程可看作<strong>参数寻优过程</strong>，即在参数空间中，寻找一组最优参数使得误差函数<strong>E</strong>最小。</p>
</li>
<li><p>两种最优: <strong>局部极小与全局最小</strong></p>
<ul>
<li>局部极小解是参数空间中的某个点，其邻域点的<strong>E</strong>均不小于该点，对应的<strong>E</strong>为误差函数的局部极小值(只需梯度为0且<strong>E</strong>小于邻点)</li>
<li>全局最小解则是指参数空间中所有点的<strong>E</strong>均不小于该点的误差函数值(最优)。</li>
</ul>
</li>
<li><p><strong>参数寻优方法</strong>-基于梯度的搜索: 从某些初始解出发迭代寻找最优参数值，每次迭代中先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向(如: 梯度下降法-沿负梯度方向)，但会陷入局部极小。<br><strong>解决办法</strong>-接近全局最小(启发式，缺乏理论保障):<br>使用多个初始值不同的神经网络，训练后取其最小误差的参数解；模拟退火；随机梯度下降(计算梯度时加入随机因素);遗传算法</p>
</li>
</ol>
<h2 id="五、-其他常见神经网络"><a href="#五、-其他常见神经网络" class="headerlink" title="五、 其他常见神经网络"></a>五、 其他常见神经网络</h2><ol>
<li><p><strong>RBF网络</strong>-径向基函数网络-单隐层前馈神经网络-与BP网络类似。使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。</p>
</li>
<li><p><strong>ART网络</strong>(自适应谐振理论网络)</p>
<ul>
<li><strong>竞争性学习</strong>: 一种无监督学习策略。输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元被抑制。即”胜者通吃”原则。</li>
<li>该网络由比较层、识别层、识别阈值和重置模块构成。比较层接收输入样本，并将其传递给识别层神经元，识别层每个神经元对应一个模式类(可动态增长)。之后识别层神经元之间相互竞争(输入向量与模式类的代表向量的距离)以产生获胜神经元。若两向量相似度大于识别阈值，则归为该代表向量一类，同时更新连接权以便之后相似样本能产生更大相似度。反之，则重置模块在识别层增设一个神经元，其代表向量设置为当前输入向量。</li>
<li>有效缓解竞争型学习中的”可塑性-稳定性窘境”，即增量学习和在线学习。</li>
</ul>
</li>
<li><p><strong>SOM网络</strong>(自组织映射网络): 一种竞争学习型的无监督神经网络。能将高维输入数据映射到低维空间(通常为二<br>维)，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到输出层中的邻近神经元。</p>
</li>
<li><p><strong>级联相关网络</strong>: 一种结构自适应网络。将网络结构当作学习的目标之一，并希望能在训练过程中找到最符合数据特点的网络结构。<br><img src="https://s2.ax1x.com/2020/03/08/3xUpbq.png" alt="3xUpbq.png"></p>
</li>
<li><p><strong>Elman网络</strong>: 一种递归神经网络(与前馈对应)。网络中可出现环形结构，从而让一些神经元的输出反馈回来作为输入信号，</p>
</li>
<li><p><strong>Boltzmann机</strong>: 一种基于能量的模型。为网络状态定义一个能量，能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。神经元(布尔型)分为两层:显层与隐层。显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达。</p>
</li>
</ol>
<h2 id="六、-深度学习"><a href="#六、-深度学习" class="headerlink" title="六、 深度学习"></a>六、 深度学习</h2><ol>
<li><p>典型的<strong>深度学习模型</strong>就是很深层的神经网络-增加隐层数目和每隐层神经元数目-通过多层处理，逐渐将初始的低层特征表示转化为高层特征表示后，用简单模型即可完成复杂的分类等学习任务。<br>节省训练开销的方法: 无监督逐层训练/权共享</p>
</li>
<li><p><strong>无监督逐层训练</strong>是多隐层网络训练的有效手段。基本思想是每次训练一层隐结点，将上一层隐结点的输出作为输入，而本层隐结点的输出作为下一层的输入，这称为预训练;在预训练全部完成后，再对整个网络进行微调训练。<br>即将大量参数分组，对每组先找到局部看来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优。</p>
</li>
<li><p><strong>权共享</strong>: 让一组神经元使用相同的连接权。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量机-机器学习</title>
    <url>/2020/03/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A06/</url>
    <content><![CDATA[<h2 id="一、间隔与支持向量"><a href="#一、间隔与支持向量" class="headerlink" title="一、间隔与支持向量"></a>一、间隔与支持向量</h2><ol>
<li><p>分类学习最基本的想法是在样本空间中找到一个<strong>划分超平面</strong>，将不同类别的样本分开。(位于不同类样本正中间的划分超平面，泛化能力最强)</p>
<a id="more"></a>
<p>划分超平面: $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b=0$</p>
</li>
<li><p>若超平面能正确分类，则有</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{ll}
\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \geqslant+1, & y_{i}=+1 \\
\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \leqslant-1, & y_{i}=-1
\end{array}\right.</script><p><strong>支持向量</strong>: 距离超平面最近的满足上式子的几个训练样本点。<br><strong>间隔</strong>: 两个异类支持向量到超平面的距离之和，即$\gamma=\frac{2}{|\boldsymbol{w}|}$</p>
</li>
<li><p><strong>支持向量机(SVM)的基本型</strong>(即求解$\boldsymbol{w}$、<strong>b</strong>，从而找到具有最大间隔的划分超平面)：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\max _{\boldsymbol{w}, b} \frac{2}{\|\boldsymbol{w}\|}\\
&\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m
\end{aligned}</script></li>
</ol>
<h2 id="二、-对偶问题"><a href="#二、-对偶问题" class="headerlink" title="二、 对偶问题"></a>二、 对偶问题</h2><p>对于SVM的基本型:<br>拉格朗日函数为 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}|\boldsymbol{w}|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)$</p>
<p>令其偏导为零，可得: </p>
<script type="math/tex; mode=display">\begin{aligned} \boldsymbol{w} &=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \\ 0 &=\sum_{i=1}^{m} \alpha_{i} y_{i} \end{aligned}</script><p>代进$L(\boldsymbol{w}, b, \boldsymbol{\alpha})$，可得到<strong>对偶问题</strong>:</p>
<script type="math/tex; mode=display">\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}</script><p>s.t. $\sum_{i=1}^{m} \alpha_{i} y_{i}=0$<br>$\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m$<br>解出$\boldsymbol{\alpha}$后，求出$\boldsymbol{w}$与b即可。</p>
<p>同时上述过程需满足基本型的约束，即<strong>KKT</strong>(Karush-Kuhn-Tucker)条件:</p>
<script type="math/tex; mode=display">\left\{\begin{array}{l}
\alpha_{i} \geqslant 0 \\
y_{i} f\left(\boldsymbol{x}_{i}\right)-1 \geqslant 0 \\
\alpha_{i}\left(y_{i} f\left(\boldsymbol{x}_{i}\right)-1\right)=0
\end{array}\right.</script><p>(可直接从不等式约束优化理解)<br>即: 最终模型仅与支持向量有关</p>
<p>求解对偶问题(二次规划)的算法: <strong>SMO</strong></p>
<h2 id="三、-核函数"><a href="#三、-核函数" class="headerlink" title="三、 核函数"></a>三、 核函数</h2><ol>
<li><p>训练样本不是线性可分的-将样本$x$从原始空间<strong>映射</strong>到一个更高维的特征空间(映射后的特征向量为$\phi(\boldsymbol{x})$)，使其在此特征空间内线性可分(有限维定存在)。</p>
</li>
<li><p>求解对偶问题时，引入<strong>核函数</strong>(设想): $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\langle\phi\left(\boldsymbol{x}_{i}\right), \phi\left(\boldsymbol{x}_{j}\right)\right\rangle=\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)$<br>解得(<strong>支持向量展式</strong>):</p>
<script type="math/tex; mode=display">\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)+b \end{aligned}</script></li>
<li><p>只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射$\phi(\boldsymbol{x})$。换言之，任何一个核函数都隐式地定义了一个称为再生核希尔伯特空间(RKHS)的特征空间。</p>
</li>
<li><p>SVM性能关键因素: <strong>核函数选择</strong>(线性核、多项式核、高斯核、拉普拉斯核、Sigmoid核等)。</p>
</li>
</ol>
<h2 id="四、-软间隔与正则化"><a href="#四、-软间隔与正则化" class="headerlink" title="四、 软间隔与正则化"></a>四、 软间隔与正则化</h2><ol>
<li><p>很难确定合适的核函数使得训练样本在特征空间中线性可分/很难识别过拟合-允许支持向量机在一些样本上出错,即<strong>软间隔</strong>。</p>
</li>
<li><p>优化目标可写为: </p>
<script type="math/tex; mode=display">\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell\left(y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)-1\right)</script><p>其中$\ell$为(替代)<strong>损失函数</strong>(hinge/指数/对数损失)。</p>
</li>
<li><p><strong>软间隔支持向量机</strong>(常见): 采用hinge损失($\ell_{\text {hinge}}(z)=\max (0,1-z)$),同时引入松弛变量$\xi_{i} \geqslant 0$，优化问题为:</p>
<script type="math/tex; mode=display">\min _{\boldsymbol{w}, b, \boldsymbol{\xi}_{i}} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}</script><script type="math/tex; mode=display">\begin{aligned}
&\text { s.t. } \quad y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1-\xi_{i}\\
&\xi_{i} \geqslant 0, i=1,2, \dots, m
\end{aligned}</script><p>(仍是二次规划问题，通过拉格朗日乘子法化为对偶问题后求解，最终模型仍仅与支持向量有关)</p>
</li>
<li><p>优化问题的一般形式，即<strong>正则化问题</strong>: </p>
<script type="math/tex; mode=display">\min _{f} \Omega(f)+C \sum_{i=1}^{m} \ell\left(f\left(\boldsymbol{x}_{i}\right), y_{i}\right)</script><p>第一项为<strong>结构风险</strong>，用于描述模型的某些性质，表述了我们希望获得具有何种性质的模型；同时该项有助于削减假设空间，从而降低了最小化训练误差的过拟合风险，因此又称为正则化项(折中项C为正则化系数，常用$\mathbf{L}_{p}$范数)。<br>第二项(不包括C)为<strong>经验风险</strong>，用于描述模型与训练数据的契合程度。</p>
</li>
</ol>
<h2 id="五、-支持向量回归"><a href="#五、-支持向量回归" class="headerlink" title="五、 支持向量回归"></a>五、 支持向量回归</h2><ol>
<li><p>支持向量回归(<strong>SVR</strong>)仅当f(x)与y之间的差别绝对值大于$\epsilon$时才计算损失，即容忍二者之间有一定偏差。<br><img src="https://s1.ax1x.com/2020/03/12/8mSbY4.png" alt="8mSbY4.png"></p>
</li>
<li><p>SVR问题可形式化为: $\min _{\boldsymbol{w}, b} \frac{1}{2}|\boldsymbol{w}|^{2}+C \sum_{i=1}^{m} \ell_{\epsilon}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}\right)$<br>$\ell_{\epsilon}$为$\epsilon$-不敏感损失函数。<br>类似的，通过拉格朗日乘子法化为对偶问题后求解，解得:</p>
<script type="math/tex; mode=display">f(\boldsymbol{x})=\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right) \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)+b</script><p>其中的$\alpha$为拉格朗日乘子。</p>
</li>
</ol>
<h2 id="六、-核方法"><a href="#六、-核方法" class="headerlink" title="六、 核方法"></a>六、 核方法</h2><ol>
<li><p><img src="https://s1.ax1x.com/2020/03/12/8m9oiF.png" alt="8m9oiF.png"></p>
</li>
<li><p><strong>核方法</strong>: 一系列基于核函数的学习方法。如，通过核化(即引入核函数)将线性学习器拓展为非线性学习器。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树-机器学习</title>
    <url>/2020/02/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A04/</url>
    <content><![CDATA[<h2 id="一、-基本流程"><a href="#一、-基本流程" class="headerlink" title="一、 基本流程"></a>一、 基本流程</h2><ol>
<li><p>根节点(样本全集)-内部节点(属性测试)-叶节点(决策结果)<br><strong>决策树桩</strong>: 仅有一层划分的决策树。</p>
</li>
<li><p><strong>决策树学习基本算法</strong></p>
<a id="more"></a>
<p><img src="https://s2.ax1x.com/2020/02/27/3wV0hD.png" alt="3wV0hD.png"></p>
<ul>
<li>典型算法: C4.5决策树</li>
<li>同一属性可隔代重复选择。</li>
</ul>
</li>
</ol>
<h2 id="二、-最优划分属性选择"><a href="#二、-最优划分属性选择" class="headerlink" title="二、 最优划分属性选择"></a>二、 最优划分属性选择</h2><ol>
<li><p><strong>信息增益</strong></p>
<ul>
<li><strong>信息熵</strong>: 度量样本集合纯度。(值越小越纯)<br>$\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_{k} \log _{2} p_{k}$，第k类样本比例为$p_{k}$</li>
<li>用属性a进行划分的<strong>信息增益</strong>:(值越大，纯度提升越大)<br>$\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)$,$D^{v}$为离散属性a取$a^{v}$的分支节点(a有V个可能取值)。</li>
<li>选择最优划分属性: $a_{*}=\underset{a \in A}{\arg \max } \operatorname{Gain}(D, a)$(对可取值数目较多的属性有所偏好)</li>
</ul>
</li>
<li><p><strong>增益率</strong></p>
<ul>
<li>对V较小的属性有所偏好: Gain ratio $(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}$</li>
<li>属性a的<strong>固有值</strong>(V越大，值越大): $\operatorname{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}$</li>
<li><strong>启发式</strong>: 先找出信息增益高于平均水平的属性，再从中选择增益率最高的。</li>
</ul>
</li>
<li><p><strong>基尼指数</strong></p>
<ul>
<li><p><strong>基尼值</strong>(值越小越纯): </p>
<script type="math/tex; mode=display">\begin{aligned} \operatorname{Gini}(D) &=\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\ &=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2} \end{aligned}</script></li>
<li><strong>基尼指数</strong>(越小越好): Gini_index$(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)$</li>
<li>选择最优划分属性: $a_{*}=\underset{a \in A}{\arg \min }$ Gini-index $(D, a)$</li>
</ul>
</li>
</ol>
<h2 id="三、-剪枝处理"><a href="#三、-剪枝处理" class="headerlink" title="三、 剪枝处理"></a>三、 剪枝处理</h2><ol>
<li><p>剪枝-预剪枝/后剪枝-去掉部分分支-<strong>降低过拟合</strong>-使决策树泛化性能提升-本节使用留出法进行性能评估，使用信息增益准则进行划分属性选择。</p>
</li>
<li><p><strong>预剪枝</strong>: 在结点划分前进行估计，若该结点的划分不能使决策树泛化性能提升，则停止划分并将该结点标记为叶结点。(节约时间，但禁止当前分支的后续划分可导致欠拟合)</p>
</li>
<li><p><strong>后剪枝</strong>: 先生成完整的决策树，然后自底向上对非叶结点考察，若将该结点对应的子树替换为叶结点能使决策树泛化性能提升，则将该子树替换为叶结点。(耗时多，但泛化性能好)</p>
</li>
</ol>
<h2 id="四、-连续与缺失值"><a href="#四、-连续与缺失值" class="headerlink" title="四、 连续与缺失值"></a>四、 连续与缺失值</h2><ol>
<li><strong>连续值处理</strong>-连续属性离散化技术-二分法(最简单): 将所有样本的属性a的取值从小到大排序，其相邻两点的平均值t为候选划分点，再按离散属性值一样，选取最优划分点进行划分，例如:<br>使信息增益最大化</li>
</ol>
<script type="math/tex; mode=display">\begin{aligned} \operatorname{Gain}(D, a) &=\max _{t \in T_{a}} \operatorname{Gain}(D, a, t) \\ &=\max _{t \in T_{a}} \operatorname{Ent}(D)-\sum_{\lambda \in\{-,+\}} \frac{\left|D_{t}^{\lambda}\right|}{|D|} \operatorname{Ent}\left(D_{t}^{\lambda}\right) \end{aligned}</script><p>注意: 若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性。</p>
<ol>
<li><strong>缺失值处理</strong>: <ul>
<li>根据没有缺失属性a的样本子集$\tilde{D}$来判断属性a的优劣，从而进行划分属性选择。</li>
<li>给定划分属性，如何对缺失该属性值的样本进行划分(用信息增益):<br>假定为每个样本x赋予权重$w_{x}$(一般恒为1)，并定义(比例)<script type="math/tex; mode=display">
\begin{aligned}
\rho &=\frac{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in D} w_{\boldsymbol{x}}} \\
\tilde{p}_{k} &=\frac{\sum_{\boldsymbol{x} \in \tilde{D}_{k}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}} \quad(1 \leqslant k \leqslant|\mathcal{Y}|) \\
\tilde{r}_{v} &=\frac{\sum_{\boldsymbol{x} \in \tilde{D}^{v}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \bar{D}} w_{\boldsymbol{x}}} \quad(1 \leqslant v \leqslant V)
\end{aligned}</script>(v-属性a取值为v，k-第k类)<br>信息增益推广为: </li>
</ul>
</li>
</ol>
<script type="math/tex; mode=display">\begin{aligned} \operatorname{Gain}(D, a) &=\rho \times \operatorname{Gain}(\tilde{D}, a) \\ &=\rho \times\left(\operatorname{Ent}(\tilde{D})-\sum_{v=1}^{V} \tilde{r}_{v} \operatorname{Ent}\left(\tilde{D}^{v}\right)\right) \end{aligned}</script><p>信息熵为: $\operatorname{Ent}(\tilde{D})=-\sum_{k=1}^{|\mathcal{Y}|} \tilde{p}_{k} \log _{2} \tilde{p}_{k}$<br>将x同时划入所有子结点，且权值在与属性值$a^{v}$对应的子结点中调整为$\tilde{r}_{v} \cdot w_{x}$，即是让同一样本以不同概率划入到不同的子结点中去。</p>
<h2 id="五、-多变量决策树"><a href="#五、-多变量决策树" class="headerlink" title="五、 多变量决策树"></a>五、 多变量决策树</h2><ol>
<li><p>将每个属性视为一个坐标轴，则d个属性描述的样本对应d维空间中的一个点，对样本分类即是在坐标空间中寻找不同类样本之间的<strong>分类边界</strong>。<br>决策树的分类边界由若干个与坐标轴平行的分段组成。(每段划分直接对应某个属性取值)</p>
</li>
<li><p>若分类边界较为复杂，应使用斜的划分边界(甚至更复杂划分)予以简化-<strong>多变量决策树</strong>: 每个非叶结点是一个形如$\sum_{i=1}^{d} w_{i} a_{i}=t$的线性分类器，即对属性的线性组合进行测试。<br><img src="https://s2.ax1x.com/2020/03/01/3ctI6U.png" alt="3ctI6U.png"></p>
</li>
<li><p><strong>多变量决策树算法</strong>: OC1，最小二乘法，神经网络(感知机树)。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯分类器-机器学习</title>
    <url>/2020/03/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A07/</url>
    <content><![CDATA[<h2 id="一、-贝叶斯决策论"><a href="#一、-贝叶斯决策论" class="headerlink" title="一、 贝叶斯决策论"></a>一、 贝叶斯决策论</h2><ol>
<li><p><strong>贝叶斯决策论</strong>: 在所有相关概率都已知的分类任务中，如何基于这些概率和误判损失来选择最优的类别标记。</p>
<a id="more"></a>
</li>
<li><p><strong>后验概率</strong>: P(原因1|结果)，即已知结果(样本特征x)，求该原因发生(样本为c类)的概率。</p>
</li>
<li><p>令$\lambda_{i j}$表示将真实标记$c_{j}$的样本误分类为$c_{i}$所产生的损失。样本x分类为$c_{i}$的期望损失即<strong>条件风险</strong>: $R\left(c_{i} | \boldsymbol{x}\right)=\sum_{j=1}^{N} \lambda_{i j} P\left(c_{j} | \boldsymbol{x}\right)$<br><strong>任务</strong>: 寻找判定准则h，以最小化总体风险$R(h)=\mathbb{E}_{\boldsymbol{x}}[R(h(\boldsymbol{x}) | \boldsymbol{x})]$</p>
</li>
<li><p><strong>贝叶斯判定准则</strong>: 为最小化总体风险，只需在每新样本上选择能使条件风险$R\left(c | \boldsymbol{x}\right)$最小的类别标记，即</p>
<script type="math/tex; mode=display">h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \min } R(c | \boldsymbol{x})</script><p>此时$R\left(h^{*}\right)$称为贝叶斯风险。<br>最小化分类错误率的贝叶斯最优分类器为: </p>
<script type="math/tex; mode=display">h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c | \boldsymbol{x})</script><p>(误判损失为0/1值)</p>
</li>
<li><p>关键在于基于有限的训练样本集尽可能准确地估计出后验概率$P(c | \boldsymbol{x})$-两种策略: </p>
<ul>
<li><strong>判别式模型</strong>: 给定<strong>x</strong>，通过直接建模$P(c | \boldsymbol{x})$来预测c，如决策树、BP神经网络、支持向量机等。</li>
<li><strong>生成式模型</strong>: 先对联合概率分布P(<strong>x</strong>,c)建模(<strong>大数定律</strong>: 概率估值来源于训练集的频率)，然后再由此获得$P(c | \boldsymbol{x})$，即:<script type="math/tex; mode=display">P(c | \boldsymbol{x})=\frac{P(c) P(\boldsymbol{x} | c)}{P(\boldsymbol{x})}</script></li>
</ul>
</li>
<li><p>假定$P(\boldsymbol{x} | c)$具有确定的形式且被参数向量$\boldsymbol{\theta}_{c}$唯一确定，再基于训练集对参数向量进行估计-两种方法:</p>
<ul>
<li><strong>频率主义学派</strong>: 参数有客观存在的固定值，可通过优化似然函数等准则来确定参数值。如: <strong>极大似然估计</strong>(MLE)</li>
<li><strong>贝叶斯学派</strong>: 参数本身也可有分布，可假定参数服从一个先验分布，然后基于训练集来计算参数的后验分布。</li>
</ul>
</li>
</ol>
<h2 id="二、-朴素贝叶斯分类器"><a href="#二、-朴素贝叶斯分类器" class="headerlink" title="二、 朴素贝叶斯分类器"></a>二、 朴素贝叶斯分类器</h2><ol>
<li><p>朴素贝叶斯分类器采用了<strong>属性条件独立性假设</strong>: 对已知类别，假设所有属性相互独立。</p>
</li>
<li><p>表达式(贝叶斯判定准则): $h_{n b}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c) \prod_{i=1}^{d} P\left(x_{i} | c\right)$</p>
</li>
<li><p>为了避免其他属性携带的信息被训练集中未出现的属性值抹去，在估计概率值时通常要进行平滑，常用<strong>拉普拉斯修正</strong>:<br>$\begin{aligned} \hat{P}(c) &amp;=\frac{\left|D_{c}\right|+1}{|D|+N} \\ \hat{P}\left(x_{i} | c\right) &amp;=\frac{\left|D_{c, x_{i}}\right|+1}{\left|D_{c}\right|+N_{i}} \end{aligned}$</p>
</li>
<li><p>若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事先计算好存储起来，这样在进行预测时只需”查表”即可进行判别。</p>
</li>
</ol>
<h2 id="三、-半朴素贝叶斯分类器"><a href="#三、-半朴素贝叶斯分类器" class="headerlink" title="三、 半朴素贝叶斯分类器"></a>三、 半朴素贝叶斯分类器</h2><ol>
<li><p><strong>半朴素贝叶斯分类器</strong>: 适当考虑一部分属性间的相互依赖(即相关)信息。<br><strong>独依赖估计</strong>(ODE,最常用): 是假设每个属性在类别之外最多仅依赖于一个其他属性，即$P(c | \boldsymbol{x}) \propto P(c) \prod_{i=1}^{d} P\left(x_{i} | c, p a_{i}\right)$<br>其中$p a_{i}$为属性$x_{i}$所依赖的属性，即父属性(关键是确定每个属性的父属性)</p>
</li>
<li><p><strong>确定父属性</strong></p>
<ul>
<li><strong>SPODE</strong>: 假设所有属性都依赖于同一属性(超父)，然后通过交叉验证等模型选择方法来确定超父属性。</li>
<li><strong>TAN</strong>: 在最大带权生成树的基础上，将属性间依赖关系约简为树形结构(仅保留强相关属性间的依赖性)。</li>
<li><strong>AODE</strong>: 尝试将每个属性作为超父来构建SPODE，然后将其中具有足够训练数据支撑的SPODE集成起来作为最终结果。<br><img src="https://s1.ax1x.com/2020/03/19/8yMSNn.png" alt="8yMSNn.png"></li>
</ul>
</li>
<li><p><strong>kDE</strong>: 考虑属性间的高阶依赖(对多个属性的依赖)-泛化性能提高</p>
</li>
</ol>
<h2 id="四、-贝叶斯网"><a href="#四、-贝叶斯网" class="headerlink" title="四、 贝叶斯网"></a>四、 贝叶斯网</h2><ol>
<li><p><strong>贝叶斯网</strong>-信念网: 借助有向无环图(DAG)刻画属性之间的依赖关系，并用条件概率表(CPT)描述属性的联合概率分布。<br><strong>结构</strong>: $B=\langle G, \Theta\rangle$，G为有向无环图(树状)，参数$\theta_{x_{i} | \pi_{i}}=P_{B}\left(x_{i} | \pi_{i}\right)$(父结点集为属性$\pi_{i}$)<br>$\Theta$描述属性的依赖关系，包含了每个属性的条件概率表。<br>各属性的<strong>联合概率分布</strong>为$\theta_{x_{i} | \pi_{i}}$之积。<br><img src="https://s1.ax1x.com/2020/03/19/8y8EkQ.png" alt="8y8EkQ.png"><br>V型结构中，若子结点$x_{4}$取值未知，则$x_{1}$与$x_{2}$独立-边际独立性</p>
</li>
<li><p>分析条件独立性，可使用<strong>有向分离</strong>($x \perp y | z$，即条件独立)-先将有向图转变为无向图(道德图): </p>
<ul>
<li>找出所有V型结构，在两父结点之间加一条无向边(道德化)。</li>
<li>将所有有向边改为无向边。<br>去除z后，x、y分隔</li>
</ul>
</li>
<li><p><strong>学习</strong>: 根据训练数据集找出结构最恰当的贝叶斯网(最常用方法-评分搜索)，再通过对训练样本计数估计出每个结点的条件概率表。<br><strong>评分搜索</strong>: 定义一个评分函数来评估贝叶斯网与训练数据的契合程度，然后基于该评分函数(值尽可能小)来寻找结构最优的贝叶斯网。<br><strong>信息论准则</strong>: 将学习问题看作一个数据压缩任务，学习目标是找到一个能以最短编码长度描述训练数据的模型，此时编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需的字节长度。<br>常用<strong>评分函数</strong>通常基于信息论准则，选择综合编码长度最短的贝叶斯网(即最小描述长度(MDL)准则)。</p>
</li>
<li><p><strong>评分函数可写为</strong>: $s(B | D)=f(\theta)|B|-L L(B | D)$<br>其中|B|是参数个数，$f(\theta)$为表示每个参数$\theta$所需字节数，<strong>x</strong>包含类别和属性，同时B的对数似然为$L L(B | D)=\sum_{i=1}^{m} \log P_{B}\left(\boldsymbol{x}_{i}\right)$<br><strong>AIC</strong>: $f(\theta)=1$； <strong>BIC</strong>: $f(\theta)=\frac{1}{2} \log m$； <strong>极大似然估计</strong>: $f(\theta)=0$<br>快速搜索网格结构的<strong>近似解方法</strong>: 贪心法(从某个结构出发进行调整)；给结构施加约束(如: 限定为树形)</p>
</li>
<li><p><strong>推断</strong>(针对一个样本): 通过已知变量观测值(即<strong>证据</strong>)来推测待查询变量(皆包括属性和类别)。<br>最理想: 直接根据贝叶斯网定义的联合概率分布来精确计算后验概率-NP难-借助近似推断(降低精度要求，在有限时间内求得近似解)-<strong>吉布斯采样</strong>(使用马尔科夫链)<br><img src="https://s1.ax1x.com/2020/03/19/8y6mad.png" alt="8y6mad.png"></p>
</li>
</ol>
<h2 id="五、-EM算法"><a href="#五、-EM算法" class="headerlink" title="五、 EM算法"></a>五、 EM算法</h2><ol>
<li><p>当训练样本某属性变量值(隐变量<strong>Z</strong>)未知时，对模型参数$\Theta$进行估计的方法: 通过对<strong>Z</strong>计算期望，来最大化已观测数据<strong>X</strong>的对数边际似然</p>
<script type="math/tex; mode=display">L L(\Theta | \mathbf{X})=\ln P(\mathbf{X} | \Theta)=\ln \sum_{\mathbf{Z}} P(\mathbf{X}, \mathbf{Z} | \Theta)</script></li>
<li><p><strong>EM算法</strong>(估计参数隐变量；迭代): </p>
<ul>
<li>期望(E)步: 利用当前估计的参数值来计算对数似然的期望值。</li>
<li>最大化(M)步: 寻找能使E步产生的似然期望最大化的参数值。然后，新得到的参数值重新被用于E步，直至收敛到局部最优解。</li>
</ul>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>聚类-机器学习</title>
    <url>/2020/03/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A09/</url>
    <content><![CDATA[<h2 id="一、-聚类任务"><a href="#一、-聚类任务" class="headerlink" title="一、 聚类任务"></a>一、 聚类任务</h2><ol>
<li><p><strong>聚类</strong>: 应用最广的一种<strong>无监督学习</strong>(训练样本的标记信息未知)-将样本划分为若干个不相交的簇$\lambda$(类)</p>
<a id="more"></a>
</li>
<li><p>聚类既能作为一个单独过程，用于找寻数据内在的分布结构，也可作为分类等其他学习任务的前驱过程。</p>
</li>
</ol>
<h2 id="二、-性能度量"><a href="#二、-性能度量" class="headerlink" title="二、 性能度量"></a>二、 性能度量</h2><ol>
<li><p>直观上，聚类结果应簇内相似度高且簇间相似度低。</p>
</li>
<li><p>聚类性能度量有两类: </p>
<ul>
<li><strong>外部指标</strong>: 将聚类结果与某个参考模型(如: 专家划分结果)进行比较-Jaccard系数、FM系数、Rand指数(值越大越好)-通过计算两模型中每样本的簇的差异得出。</li>
<li><strong>内部指标</strong>: 直接考察聚类结果而不利用任何参考模型-DB指数(小好)、Dunn指数(大好)-通过计算每两样本的距离得出。</li>
</ul>
</li>
</ol>
<h2 id="三、-距离计算"><a href="#三、-距离计算" class="headerlink" title="三、 距离计算"></a>三、 距离计算</h2><ol>
<li><p><strong>闵可夫斯基距离</strong>(用于有序属性):</p>
<script type="math/tex; mode=display">\operatorname{dist}_{\mathrm{mk}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right|^{p}\right)^{\frac{1}{p}}</script><p>p=1即<strong>曼哈顿距离</strong>，p=2即<strong>欧氏距离</strong></p>
</li>
<li><p><strong>VDM距离</strong>(用于无序属性):</p>
<script type="math/tex; mode=display">\operatorname{VDM}_{p}(a, b)=\sum_{i=1}^{k}\left|\frac{m_{u, a, i}}{m_{u, a}}-\frac{m_{u, b, i}}{m_{u, b}}\right|^{p}</script><p>其中$m_{u, a, i}$表示在第i个样本簇中在属性u上取值为a的样本数,k为样本簇数。</p>
</li>
<li><p>二者结合可处理混合属性。<br>当属性重要性不同时可使用加权距离。<br>非度量距离不满足直递性。</p>
</li>
</ol>
<h2 id="四、-原型聚类"><a href="#四、-原型聚类" class="headerlink" title="四、 原型聚类"></a>四、 原型聚类</h2><ol>
<li><p>原型聚类: 先对原型进行初始化，然后对原型进行<strong>迭代</strong>更新求解。</p>
</li>
<li><p><strong>k均值算法</strong>: 针对聚类所得簇划分C,最小化平方误差</p>
<script type="math/tex; mode=display">E=\sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_{i}}\left\|\boldsymbol{x}-\boldsymbol{\mu}_{i}\right\|_{2}^{2}</script><p>其中$\boldsymbol{\mu}_{i}$是簇$C_{i}$的均值向量。<br>采用贪心策略，通过迭代优化来近似求解上式<br><img src="https://s1.ax1x.com/2020/03/27/Gi37s1.png" alt="Gi37s1.png"></p>
</li>
<li><p><strong>学习向量量化</strong>(LVQ): 已知数据样本的类别标记，先假设每簇预设的类别标记，再通过初始化后迭代得到Voronoi剖分与原型向量。<br><img src="https://s1.ax1x.com/2020/03/27/GiJtq1.png" alt="GiJtq1.png"></p>
</li>
<li><p><strong>高斯混合聚类</strong>(采用概率模型)</p>
<ul>
<li>高斯分布的密度函数: $p(\boldsymbol{x})=\frac{1}{(2 \pi)^{\frac{n}{2}}|\mathbf{\Sigma}|^{\frac{1}{2}}} e^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathbf{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}$<br>高斯混合分布: $p_{\mathcal{M}}(\boldsymbol{x})=\sum_{i=1}^{k} \alpha_{i} \cdot p\left(\boldsymbol{x} | \boldsymbol{\mu}_{i}, \mathbf{\Sigma}_{i}\right)$</li>
<li>假设训练集由高斯混合分布生成，我们把样本集划分为k个簇。即采用概率模型对原型进行刻画，簇划分则由原型对应后验概率确定。</li>
<li>求解模型参数(即高斯混合分布的参数): 极大似然估计<script type="math/tex; mode=display">\begin{aligned}
L L(D) &=\ln \left(\prod_{j=1}^{m} p_{\mathcal{M}}\left(\boldsymbol{x}_{j}\right)\right) \\
&=\sum_{j=1}^{m} \ln \left(\sum_{i=1}^{k} \alpha_{i} \cdot p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right)\right)
\end{aligned}</script></li>
<li>对于上式，常采用EM算法进行迭代优化求解。<br><img src="https://s1.ax1x.com/2020/03/29/GV8KUI.png" alt="GV8KUI.png"></li>
</ul>
</li>
</ol>
<h2 id="五、-密度聚类"><a href="#五、-密度聚类" class="headerlink" title="五、 密度聚类"></a>五、 密度聚类</h2><ol>
<li><p><strong>密度聚类</strong>: 从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。</p>
</li>
<li><p><strong>DBSCAN算法</strong>: 基于一组邻域参数($\epsilon, M i n P t s$)来刻画样本分布的紧密程度。<br><img src="https://s1.ax1x.com/2020/03/29/GVd0jx.png" alt="GVd0jx.png"><br><strong>簇</strong>: 由密度可达关系导出的最大的密度相连样本集合。<br><strong>过程</strong>: 先根据邻域参数找出所有核心对象，然后以任一核心对象为出发点，找出由其密度可达的样本生成聚类簇，直到所有核心对象均被访问过为止。</p>
</li>
</ol>
<h2 id="六、-层次聚类"><a href="#六、-层次聚类" class="headerlink" title="六、 层次聚类"></a>六、 层次聚类</h2><ol>
<li><p>层次聚类试图在不同层次对数据集进行划分，从而形成<strong>树形聚类结构</strong>。<br>数据集的划分可采用自底向上的<strong>聚合策略</strong>(如: AGNES)，也可采用自顶向下的<strong>分拆策略</strong>。</p>
</li>
<li><p><strong>AGNES</strong>: 先将每个样本看做初始聚类簇，然后在算法运行的每一步中找出距离最近的两个簇进行合并，该过程不断重复，直至达到预设的聚类簇个数。<br>聚类簇距离: 最小距离-单链接；最大距离-全链接；平均距离-均链接。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习-机器学习</title>
    <url>/2020/03/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A08/</url>
    <content><![CDATA[<h2 id="一、-个体与集成"><a href="#一、-个体与集成" class="headerlink" title="一、 个体与集成"></a>一、 个体与集成</h2><ol>
<li><p><strong>集成学习</strong>: 通过构建并结合多个学习器来完成学习任务。先产生一组个体学习器，再用某种策略将它们结合起来。<br>同质集成(同种类型)的个体学习器亦称<strong>基学习器</strong>(多为弱学习器), 相应的学习算法称为基学习算法。<br>异质集成的个体学习器称为<strong>组件学习器</strong>或直接称为个体学习器。</p>
<a id="more"></a>
</li>
<li><p>集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能(单一学习器要<strong>好而不同</strong>-研究核心)。<br>个体学习器的准确性和多样性存在冲突。</p>
</li>
<li><p><strong>集成学习方法</strong></p>
<ul>
<li>个体学习器间存在强依赖关系、必须串行生成的序列化方法: Boosting</li>
<li>个体学习器间不存在强依赖关系、可同时生成的并行化方法: Bagging、随机森林</li>
</ul>
</li>
</ol>
<h2 id="二、Boosting"><a href="#二、Boosting" class="headerlink" title="二、Boosting"></a>二、Boosting</h2><ol>
<li><p><strong>Boosting族算法</strong>: 先从训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。</p>
</li>
<li><p><strong>AdaBoost算法</strong>: 基学习器的线性组合$H(\boldsymbol{x})=\sum_{t=1}^{T} \alpha_{t} h_{t}(\boldsymbol{x})$，来最小化指数损失函数(基于加性模型的推导):<br>$\ell_{\exp }(H | \mathcal{D})=\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x}) H(\boldsymbol{x})}\right]$,其中f是真实函数，y取+1或-1。<br>令其对$H(\boldsymbol{x})$的偏导为0，有:</p>
<script type="math/tex; mode=display">H(x)=\frac{1}{2} \ln \frac{P(f(x)=1 | x)}{P(f(x)=-1 | x)}</script><p>从而</p>
<script type="math/tex; mode=display">\begin{aligned}
\operatorname{sign}(H(x)) &=\operatorname{sign}\left(\frac{1}{2} \ln \frac{P(f(x)=1 | x)}{P(f(x)=-1 | x)}\right) \\
&=\left\{\begin{array}{ll}
1, & P(f(x)=1 | x)>P(f(x)=-1 | x) \\
-1, & P(f(x)=1 | x)<P(f(x)=-1 | x)
\end{array}\right.\\
&=\underset{y \in\{-1,1\}}{\arg \max } P(f(x)=y | x)
\end{aligned}</script><p>意味着sign(<strong>H</strong>(<strong>x</strong>))达到了贝叶斯最优错误率。即若指数损失函数最小化，则分类错误率也将最小化，从而可用其替代0/1损失函数作为优化目标。</p>
</li>
</ol>
<p>基于初始数据分布得到$h_{1}$后，迭代生成$h_{t}$和权重$\alpha_{t}$，且权重应使得$\alpha_{t}h_{t}$最小化指数损失函数</p>
<script type="math/tex; mode=display">\ell_{\exp }\left(\alpha_{t} h_{t} | \mathcal{D}_{t}\right)=\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}_{t}}\left[e^{-f(\boldsymbol{x}) \alpha_{t} h_{t}(\boldsymbol{x})}\right]</script><script type="math/tex; mode=display">=e^{-\alpha_{t}}\left(1-\epsilon_{t}\right)+e^{\alpha_{t}} \epsilon_{t}</script><p>其中$\epsilon_{t}=P_{x \sim D_{t}}\left(h_{t}(\boldsymbol{x}) \neq f(\boldsymbol{x})\right)$<br>取其偏导为0，可得下图第6行的分类器权重更新公式。</p>
<p>AdaBoost算法在获得$H_{t-1}$之后样本分布将进行调整，使下一轮的$h_{t}能纠正一些错误。即最小化</p>
<script type="math/tex; mode=display">\ell_{\exp }\left(H_{t-1}+h_{t} | \mathcal{D}\right)=\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x})\left(H_{t-1}(\boldsymbol{x})+h_{t}(\boldsymbol{x})\right)}\right]</script><p>解得$h_{t}(\boldsymbol{x})=\underset{h}{\arg \min } \mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}_{t}}[\mathbb{I}(f(\boldsymbol{x}) \neq h(\boldsymbol{x}))]$<br>即理想的$h_{t}$将最小化分类误差。<br><img src="https://s1.ax1x.com/2020/03/23/8HEjiV.png" alt="8HEjiV.png"><em>**</em><br>从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。</p>
<h2 id="三、-Bagging与随机森林"><a href="#三、-Bagging与随机森林" class="headerlink" title="三、 Bagging与随机森林"></a>三、 Bagging与随机森林</h2><ol>
<li><p>Bagging的<strong>基本流程</strong>: 基于自助采样法，采样出T个含m个训练样本的采样集，然后基于每个采样集训练出基学习器，再使用简单投票法/平均法将基学习器进行结合。</p>
</li>
<li><p>Bagging的优点: <strong>高效</strong>-复杂度与基学习器同阶。<br>未使用的约36.8%的样本可用作验证集来对泛化性能进行<strong>包外估计</strong>。</p>
</li>
<li><p>从偏差-方差分解的角度看， Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。</p>
</li>
<li><p><strong>随机森林</strong>(RF)是Bagging的扩展变体.RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了<strong>随机属性选择</strong>-对基决策树的每个结点，先从该结点的属性集合(d个)中随机选择一个包含k个属性的子集，然后再从该子集中选择一个最优属性用于划分(一般取$k=\log _{2} d$)。</p>
</li>
<li><p>随机森林泛化性能-基学习器的多样性-<strong>样本扰动</strong>和<strong>属性扰动</strong></p>
</li>
</ol>
<h2 id="四、-结合策略"><a href="#四、-结合策略" class="headerlink" title="四、 结合策略"></a>四、 结合策略</h2><ol>
<li><p>学习器结合的好处<br><img src="https://s1.ax1x.com/2020/03/24/8LNDvF.png" alt="8LNDvF.png"></p>
</li>
<li><p><strong>平均法</strong>-对于数值型输出-简单/加权平均</p>
</li>
<li><p><strong>投票法</strong>-对于标记/概率输出-绝对多数(未过半数则拒绝)/相对多数/加权投票法</p>
</li>
<li><p><strong>学习法</strong>: 通过另一个学习器来进行结合，如:<br><strong>Stacking</strong>: 先从初始数据集训练出初级学习器，然后生成一个新数据集(用交叉验证或留一法减少过拟合)用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，初始样本的标记仍被当作样例标记。</p>
</li>
</ol>
<h2 id="五、-多样性"><a href="#五、-多样性" class="headerlink" title="五、 多样性"></a>五、 多样性</h2><ol>
<li><p><strong>误差一分歧分解</strong>(回归学习)</p>
<script type="math/tex; mode=display">E=\bar{E}-\bar{A}</script><p>(集成的泛化误差 = 个体学习器泛化误差的加权均值 - 个体学习器的加权分歧值)<br>即: 个体学习器准确性越高、多样性越大，则集成越好。</p>
</li>
<li><p><strong>多样性度量</strong>-个体分类器的多样性-不合度量/相关系数/Q-统计量/$\kappa$-统计量<br>$\kappa$<strong>-误差图</strong>: 将每一对分类器作为图上的一点，横坐标是这对分类器的$\kappa$值(多样性)，纵坐标是它们的平均误差(准确性)。</p>
</li>
<li><p><strong>多样性增强</strong>: 引入随机项-对数据样本、输入属性、输出表示、算法参数进行扰动。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>美式英语发音规则</title>
    <url>/2020/03/25/%E7%BE%8E%E5%BC%8F%E8%8B%B1%E8%AF%AD%E5%8F%91%E9%9F%B3%E8%A7%84%E5%88%99/</url>
    <content><![CDATA[<h2 id="一、-辅-元-连读"><a href="#一、-辅-元-连读" class="headerlink" title="一、 辅 + 元 连读"></a>一、 辅 + 元 连读</h2><p>特例: /r/、/l/、/ŋ/重复<br><a id="more"></a></p>
<h2 id="二、-元-元-添加连读"><a href="#二、-元-元-添加连读" class="headerlink" title="二、 元 + 元 添加连读"></a>二、 元 + 元 添加连读</h2><p>o、u-尾音/u/: 元(w)元<br>a、e、i-尾音/i/: 元(j)元<br>如:<br>he and I /hi (j)ə nai/<br>you and I /ju (w)ə nai/</p>
<p>特例一: 单词内部<br>doing /du(w)iŋ/<br>fire /fai(j)ər/</p>
<p>特例二(补充):<br>/n/ + /s/ = /n ts/ 送气<br>/n/ + /z/ = /n dz/ 不送气</p>
<h2 id="三、-辅-辅-略读"><a href="#三、-辅-辅-略读" class="headerlink" title="三、 辅 + 辅 略读"></a>三、 辅 + 辅 略读</h2><p>/t/和/d/，/p/和/b/，/k/和/g/省前发后(异同、双向都可)</p>
<p>浊+清，省浊(v和f、z和s、ð和θ、ʒ和ʃ)</p>
<p>/tʃ/和/dʒ/不省(异同)</p>
<p>nt + 元音，非重读时省略t</p>
<h2 id="四、-辅-辅-吞音"><a href="#四、-辅-辅-吞音" class="headerlink" title="四、 辅 +辅 吞音"></a>四、 辅 +辅 吞音</h2><p>/t/、/d/、/p/、/b/、/k/、/g/ + 辅(非兄弟)，做口型不发声</p>
<h2 id="五、-吞音、喉舌停顿"><a href="#五、-吞音、喉舌停顿" class="headerlink" title="五、 吞音、喉舌停顿"></a>五、 吞音、喉舌停顿</h2><p>/tən/ → /ən/<br>/tɪn/ → /ɪn/</p>
<h2 id="六、-略读-H-dropping"><a href="#六、-略读-H-dropping" class="headerlink" title="六、 略读 H-dropping"></a>六、 略读 H-dropping</h2><p>即/h/不发音<br>常见: he、his、him、her</p>
<h2 id="七、-略读-G-dropping"><a href="#七、-略读-G-dropping" class="headerlink" title="七、 略读 G-dropping"></a>七、 略读 G-dropping</h2><p>ing: /iŋ/或者/ɪn’/</p>
<h2 id="八、-变音"><a href="#八、-变音" class="headerlink" title="八、 变音"></a>八、 变音</h2><p>辅音 + /j/:<br>/t/ + /ju/ = /tʃu/<br>/d/ + /ju/ = /dʒu/<br>/k/ + /ju/ = /kju/</p>
<p>辅音+/w/: 理解成快速连读</p>
<h2 id="九、-弱读"><a href="#九、-弱读" class="headerlink" title="九、 弱读"></a>九、 弱读</h2><p>can-句首或肯定: /cən/</p>
<p>have-非实义动词: /həv/</p>
<p>the-后无元音: /ðə/</p>
<p>单元音 + r: /ər/</p>
<h2 id="十、-其它"><a href="#十、-其它" class="headerlink" title="十、 其它"></a>十、 其它</h2><p>s后面的清辅音(后接元音)浊化，如: speak、stop、sport</p>
<p>wanna /wʌnnə/ = want to<br>gonna /gʌnə/ = going to<br>gotta /gʌdə/ = have got to</p>
<p>kind of = kinda<br>sort of = sorta<br>a lot of = lotta<br>could of = coulda<br>would of = woulda<br>should of = shoulda</p>
]]></content>
      <tags>
        <tag>技能</tag>
      </tags>
  </entry>
</search>
